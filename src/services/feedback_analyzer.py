"""Feedback analyzer service."""

import json
import logging
import re
from typing import TYPE_CHECKING, Any

from src.core.prompts import PromptBuilder
from src.schemas.request import ResumeAnalysisRequest
from src.schemas.response import (
    CoreValueScore,
    FeedbackItem,
    FrequentInterviewQuestion,
    ImprovementItem,
    InterviewDetailInfo,
    InterviewQuestion,
    KeywordAnalysis,
    LengthCheck,
    NCSAnalysis,
    NCSCompetencyScore,
    NCSItem,
    NewResumeAnalysisResponse,
    OrganizationInfo,
    PastQuestion,
    PositionSkillMatch,
    RecentNewsItem,
    ResumeAnalysisResponse,
    SimilarQuestion,
    StrengthItem,
    TalentAnalysis,
    WarningItem,
)

if TYPE_CHECKING:
    from src.data.interview_manager import InterviewManager
    from src.data.organization_manager import OrganizationManager
    from src.data.position_manager import PositionManager
    from src.services.llm_service import LLMService

logger = logging.getLogger(__name__)


# Pre-compiled regex patterns for performance (cached at module level)
# ë¸”ë¼ì¸ë“œ ì±„ìš© ìœ„ë°˜ ì²´í¬ìš© íŒ¨í„´
_SCHOOL_PATTERNS = [
    (re.compile(r"[ê°€-í£]+ëŒ€í•™êµ"), "í•™êµëª…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤"),
    (re.compile(r"[ê°€-í£]+ëŒ€í•™"), "í•™êµëª…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤"),
    (re.compile(r"(ì„œìš¸|ì—°ì„¸|ê³ ë ¤|ì„±ê· ê´€|í•œì–‘|ì¤‘ì•™|ê²½í¬|í•œêµ­ì™¸êµ­ì–´|ì„œê°•|ì´í™”|ìˆ™ëª…|ê±´êµ­|ë™êµ­|í™ìµ|êµ­ë¯¼|ì„¸ì¢…|ë‹¨êµ­|ì¸í•˜|ì•„ì£¼|ê²½ë¶|ë¶€ì‚°|ì „ë‚¨|ì „ë¶|ì¶©ë‚¨|ì¶©ë¶|ê°•ì›|ì œì£¼)ëŒ€"), "í•™êµëª…ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤"),
]
_REGION_PATTERNS = [
    (re.compile(r"(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)(\s)?(ì¶œì‹ |íƒœìƒ|ì—ì„œ íƒœì–´|ì—ì„œ ìë¼)"), "ì¶œì‹  ì§€ì—­ì´ ì–¸ê¸‰ë˜ì–´ ìˆìŠµë‹ˆë‹¤"),
]
_FAMILY_KEYWORDS_SHORT = [
    (re.compile(r"\bí˜•\b"), "í˜•"),
    (re.compile(r"\bëˆ„ë‚˜\b"), "ëˆ„ë‚˜"),
    (re.compile(r"\bì˜¤ë¹ \b"), "ì˜¤ë¹ "),
    (re.compile(r"\bì–¸ë‹ˆ\b"), "ì–¸ë‹ˆ"),
    (re.compile(r"(?<![ê°€-í£])ë™ìƒ(?![ê°€-í£])"), "ë™ìƒ"),
]
# ìˆ«ì/ìˆ˜ì¹˜ íŒ¨í„´
_NUMBER_PATTERN = re.compile(r"\d+(\.\d+)?(%|ë°°|ëª…|ê±´|ì›|ë§Œì›|ì–µ|ê°œì›”|ì£¼|ì¼|ì‹œê°„|íšŒ|ê°œ|ì |ìœ„|ë“±)")
# ê²½í—˜ í‚¤ì›Œë“œ íŒ¨í„´
_EXPERIENCE_PATTERN = re.compile(r"(í”„ë¡œì íŠ¸|ì¸í„´|ëŒ€íšŒ|ê³µëª¨ì „|ë´‰ì‚¬|ë™ì•„ë¦¬|íŒ€|ë¦¬ë”|ë‹´ë‹¹|ìˆ˜í–‰|ê°œë°œ|ë¶„ì„|ê¸°íš|ì§„í–‰)")
# ë¬¸ì¥ ë¶„ë¦¬ íŒ¨í„´
_SENTENCE_SPLIT_PATTERN = re.compile(r"[.!?]\s*")
# ê²°ê³¼/ì„±ê³¼ í‘œí˜„ íŒ¨í„´
_RESULT_PATTERNS = [
    re.compile(r"ê²°ê³¼[,\s]"),
    re.compile(r"ì„±ê³¼[ë¥¼ê°€]"),
    re.compile(r"ë‹¬ì„±"),
    re.compile(r"í–¥ìƒ"),
    re.compile(r"ê°œì„ "),
    re.compile(r"ì¦ê°€"),
    re.compile(r"ê°ì†Œ"),
    re.compile(r"ì ˆê°"),
    re.compile(r"ìˆ˜ìƒ"),
    re.compile(r"ì„ ì •"),
]
# ê°€ì¡± ë§¥ë½ ì²´í¬ìš© íŒ¨í„´ - ì‹¤ì œ ì°¨ë³„ ìš”ì†Œë§Œ ì²´í¬
# ë¸”ë¼ì¸ë“œ ì±„ìš©ì—ì„œ ê¸ˆì§€í•˜ëŠ” ê²ƒ: ë¶€ëª¨ ì§ì—…/ì¬ì‚°/í•™ë ¥/ì‚¬íšŒì  ì§€ìœ„ ë“± ì°¨ë³„ ìš”ì†Œ
_PERSONAL_CONTEXT_PATTERNS = [
    re.compile(r"(ì§ì—…ì´|ì§ì—…ì„|ì§ì—…ì€|ì¼ì„\s*í•˜ì‹œ|íšŒì‚¬ì—ì„œ\s*ì¼|ì‚¬ì—…ì„|ìš´ì˜í•˜ì‹œ|ê·¼ë¬´í•˜ì‹œ)"),  # ë¶€ëª¨ ì§ì—…
    re.compile(r"(ê²½ì œì |ê°€ë‚œí–ˆ|ë¶€ìœ í–ˆ|ì–´ë ¤ìš´\s*í™˜ê²½|ë„‰ë„‰í•œ\s*í™˜ê²½|í˜•í¸ì´)"),  # ê²½ì œ ìƒí™©
    re.compile(r"(í•™ë ¥|í•™ë²Œ|ëŒ€í•™ì„\s*ë‚˜ì˜¤|ì¡¸ì—…í•˜ì‹ )"),  # ë¶€ëª¨ í•™ë ¥
    re.compile(r"(ì¬ì‚°|ìœ ì‚°|ë¬¼ë ¤ë°›|ìƒì†)"),  # ì¬ì‚°/ìƒì†
    re.compile(r"(ì‚¬íšŒì \s*ì§€ìœ„|ì¸ë§¥|ì—°ì¤„)"),  # ì‚¬íšŒì  ì§€ìœ„
    re.compile(r"(ëŒì•„ê°€ì‹œ|ë³„ì„¸í•˜|ì‚¬ë§í•˜)"),  # ì‚¬ë§ ê´€ë ¨ (ë™ì •ì‹¬ ìœ ë°œ)
]
# ì˜ˆì™¸ íŒ¨í„´ - ë¹„ìœ ì /ê´€ìš©ì  í‘œí˜„ ë° ê°€ì¹˜ê´€/êµí›ˆ ê´€ë ¨ (ë¸”ë¼ì¸ë“œ ìœ„ë°˜ ì•„ë‹˜)
_EXCEPTION_PATTERNS = [
    # ë¹„ìœ ì  í‘œí˜„
    re.compile(r"ì•„ë²„ì§€ëŠ”\s*ì‚°"),
    re.compile(r"ì–´ë¨¸ë‹ˆëŠ”\s*(ê°•|ë°”ë‹¤)"),
    re.compile(r"ì•„ë²„ì§€\s*ê°™ì€"),
    re.compile(r"ì–´ë¨¸ë‹ˆ\s*ê°™ì€"),
    re.compile(r"ë¶€ëª¨\s*ê°™ì€"),
    re.compile(r"í˜•\s*ê°™ì€"),
    re.compile(r"ëˆ„ë‚˜\s*ê°™ì€"),
    re.compile(r"ë™ìƒ\s*ê°™ì€"),
    re.compile(r"ê°€ì¡±\s*(ê°™ì€|ì²˜ëŸ¼)"),
    re.compile(r"í˜•ì œ\s*(ê°™ì€|ì²˜ëŸ¼)"),
    re.compile(r"ê³ í–¥\s*(ê°™ì€|ì²˜ëŸ¼)"),
    # ê´€ìš©ì  í‘œí˜„
    re.compile(r"ì¸ì‹¬"),
    re.compile(r"ì‚¬ëŒ\s*ì‚¬ëŠ”\s*ì •"),
    re.compile(r"ì •ì´\s*ë§"),
    re.compile(r"ë”°ëœ»í•œ\s*ë§ˆìŒ"),
    # ê°€ì¹˜ê´€/êµí›ˆ ê´€ë ¨ (ë¸”ë¼ì¸ë“œ ìœ„ë°˜ ì•„ë‹˜ - ì°¨ë³„ ìš”ì†Œê°€ ì•„ë‹Œ ì¸ì„±/ê°€ì¹˜ê´€)
    re.compile(r"ê°€í›ˆ"),  # ê°€í›ˆ
    re.compile(r"ê°€ì¹˜ê´€"),  # ê°€ì¹˜ê´€
    re.compile(r"êµí›ˆ"),  # êµí›ˆ
    re.compile(r"ì¢Œìš°ëª…"),  # ì¢Œìš°ëª…
    re.compile(r"ì‹ ë…"),  # ì‹ ë…
    re.compile(r"ì² í•™"),  # ì² í•™
    re.compile(r"ë§ˆìŒê°€ì§"),  # ë§ˆìŒê°€ì§
    re.compile(r"ì‚¶ì˜\s*(ìì„¸|íƒœë„|ë°©ì‹)"),  # ì‚¶ì˜ ìì„¸
    re.compile(r"(ì‚¬ë‘|ë´‰ì‚¬|ë‚˜ëˆ”|ë°°ë ¤|ì¡´ì¤‘|ì •ì§|ì„±ì‹¤|ì±…ì„|ê°ì‚¬).*ë°”íƒ•"),  # ê°€ì¹˜ ê¸°ë°˜
    re.compile(r"ì£¼ì‹ \s*(ë§ì”€|ê°€ë¥´ì¹¨).*ë°”íƒ•"),  # ê°€ë¥´ì¹¨ ë°”íƒ•ìœ¼ë¡œ
    re.compile(r"(ë§ì”€|ê°€ë¥´ì¹¨).*ë”°ë¼"),  # ê°€ë¥´ì¹¨ì„ ë”°ë¼
    re.compile(r"ë¬¼ë ¤ë°›ì€\s*(ì •ì‹ |ë§ˆìŒ|ê°€ì¹˜)"),  # ì •ì‹ ì  ìœ ì‚° (ì¬ì‚° ì•„ë‹˜)
]
_DIRECT_FAMILY_PATTERNS = [
    (re.compile(r"ê°€ì •í™˜ê²½ì´|ê°€ì •\s*í˜•í¸ì´|ì§‘ì•ˆ\s*í˜•í¸"), "ê°€ì •í™˜ê²½/ì§‘ì•ˆ í˜•í¸ì— ëŒ€í•œ ì–¸ê¸‰ì´ ìˆìŠµë‹ˆë‹¤"),
]
# ê¸°ê´€ëª… ì²´í¬ íŒ¨í„´ (IGNORECASE)
_ORG_NAME_PATTERNS = [
    (re.compile(r"í•œêµ­ì „ë ¥|í•œì „", re.IGNORECASE), "KEPCO"),
    (re.compile(r"í•œêµ­ê°€ìŠ¤ê³µì‚¬|ê°€ìŠ¤ê³µì‚¬", re.IGNORECASE), "KOGAS"),
    (re.compile(r"í•œêµ­ìˆ˜ìì›ê³µì‚¬|ìˆ˜ìì›ê³µì‚¬|K-water", re.IGNORECASE), "KWATER"),
    (re.compile(r"í•œêµ­ì² ë„ê³µì‚¬|ì½”ë ˆì¼", re.IGNORECASE), "KORAIL"),
    (re.compile(r"í•œêµ­í† ì§€ì£¼íƒê³µì‚¬|LHê³µì‚¬", re.IGNORECASE), "LH"),
    (re.compile(r"êµ­ë¯¼ê±´ê°•ë³´í—˜ê³µë‹¨|ê±´ë³´ê³µë‹¨", re.IGNORECASE), "NHIS"),
    (re.compile(r"êµ­ë¯¼ì—°ê¸ˆê³µë‹¨|ì—°ê¸ˆê³µë‹¨", re.IGNORECASE), "NPS"),
    (re.compile(r"í•œêµ­ì‚°ì—…ì¸ë ¥ê³µë‹¨|ì‚°ì¸ê³µ", re.IGNORECASE), "HRDK"),
    (re.compile(r"í•œêµ­ë†ìˆ˜ì‚°ì‹í’ˆìœ í†µê³µì‚¬|aT|ì—ì´í‹°", re.IGNORECASE), "AT"),
    (re.compile(r"í•œêµ­ë„ë¡œê³µì‚¬|ë„ë¡œê³µì‚¬", re.IGNORECASE), "EX"),
]


class FeedbackAnalyzer:
    """Analyzer for resume feedback generation."""

    def __init__(
        self,
        llm_service: "LLMService",
        org_manager: "OrganizationManager",
        interview_manager: "InterviewManager | None" = None,
        position_manager: "PositionManager | None" = None,
    ) -> None:
        """Initialize feedback analyzer.

        Args:
            llm_service: LLM service for generating analysis
            org_manager: Organization data manager
            interview_manager: Interview data manager (optional)
            position_manager: Position data manager (optional)
        """
        self._llm_service = llm_service
        self._org_manager = org_manager
        self._interview_manager = interview_manager
        self._position_manager = position_manager

    async def analyze(self, request: ResumeAnalysisRequest) -> ResumeAnalysisResponse:
        """Analyze resume and generate feedback (legacy v1).

        Args:
            request: Resume analysis request

        Returns:
            Complete analysis response
        """
        # Get organization data
        org_data = self._org_manager.get_organization(request.organization)

        # Get interview data if available
        interview_data = self._get_interview_data(request.organization)

        # Build prompts with position and interview context
        system_prompt = PromptBuilder.build_system_prompt(
            org_data=org_data,
            position=request.position,
            interview_data=interview_data,
        )
        user_prompt = PromptBuilder.build_user_prompt(request)

        # Get LLM analysis
        llm_response = await self._llm_service.analyze(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            json_mode=True,
        )

        # Parse and validate response
        return self._parse_response(llm_response, request)

    async def analyze_v2(
        self, request: ResumeAnalysisRequest
    ) -> NewResumeAnalysisResponse:
        """Analyze resume and generate feedback (v2 - redesigned).

        Args:
            request: Resume analysis request

        Returns:
            New format analysis response
        """
        # Get organization data
        org_data = self._org_manager.get_organization(request.organization)

        # Get interview data if available
        interview_data = self._get_interview_data(request.organization)

        # Get past questions data
        past_questions_data = self._get_past_questions(request.organization)

        # Pre-compute analyses for prompt enhancement (ê¸°ê´€ë³„ ë§ì¶¤í˜• ëª¨ë²”ë‹µì•ˆ ìƒì„±ìš©)
        core_values = org_data.get("core_values", []) if org_data else []
        core_value_scores = self.analyze_core_values(request.answer, core_values)
        ncs_competency_scores = self.analyze_ncs_competencies(request.answer, request.position)
        position_skill_match = self.analyze_position_skills(request.answer, request.position)

        # Build prompts with new v2 format (include pre-computed analyses)
        system_prompt = PromptBuilder.build_system_prompt_v2(
            org_data=org_data,
            position=request.position,
            interview_data=interview_data,
            core_value_scores=core_value_scores,
            ncs_competency_scores=ncs_competency_scores,
            position_skill_match=position_skill_match,
        )
        user_prompt = PromptBuilder.build_user_prompt(request)

        # Get LLM analysis
        llm_response = await self._llm_service.analyze(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            json_mode=True,
        )

        # Parse and validate response (pass pre-computed analyses)
        return self._parse_response_v2(
            llm_response, request, org_data, interview_data, past_questions_data,
            core_value_scores, ncs_competency_scores, position_skill_match,
        )

    def _get_interview_data(self, organization: str) -> dict[str, Any] | None:
        """Get interview data for organization if available.

        Args:
            organization: Organization code

        Returns:
            Interview data or None if not available
        """
        if not self._interview_manager:
            return None

        try:
            return self._interview_manager.get_interview(organization)
        except Exception:
            # Interview data not found, return None gracefully
            return None

    def _get_past_questions(self, organization: str) -> list[dict[str, Any]]:
        """Get past essay questions for organization.

        Args:
            organization: Organization code

        Returns:
            List of past questions or empty list
        """
        # Try to load from knowledge-db questions directory
        from pathlib import Path

        questions = []
        knowledge_db_path = Path(__file__).parent.parent.parent.parent / "resume-knowledge-db" / "data" / "questions"

        if not knowledge_db_path.exists():
            return questions

        # Find question files for this organization (recent years)
        # Include both verified (past) and unverified (predicted) questions
        for year in [2026, 2025, 2024, 2023, 2022]:
            question_file = knowledge_db_path / f"{organization.upper()}_{year}.json"
            if question_file.exists():
                try:
                    import json
                    with open(question_file, encoding="utf-8") as f:
                        data = json.load(f)
                        metadata = data.get("metadata", {})
                        is_verified = metadata.get("verified", True)
                        for q in data.get("questions", [])[:2]:  # Max 2 per year
                            questions.append({
                                "year": year,
                                "half": data.get("half", ""),
                                "question": q.get("text", ""),
                                "char_limit": q.get("char_limit", 0),
                                "is_prediction": not is_verified,  # True if not verified (prediction)
                            })
                except Exception:
                    pass

        return questions[:6]  # Max 6 total

    def _parse_response(
        self,
        llm_response: str,
        request: ResumeAnalysisRequest,
    ) -> ResumeAnalysisResponse:
        """Parse LLM response into structured response.

        Args:
            llm_response: Raw LLM JSON response
            request: Original request for length calculation

        Returns:
            Validated response object
        """
        try:
            data = json.loads(llm_response)
        except json.JSONDecodeError as e:
            logger.error(f"LLM returned invalid JSON: {e}")
            logger.debug(f"Raw response: {llm_response[:500]}...")
            # Return default response on parse error
            data = {
                "overall_score": 50,
                "feedbacks": [],
                "keyword_analysis": {"found_keywords": [], "missing_keywords": [], "match_rate": 0},
                "expected_questions": [],
            }

        # Calculate actual length check (override LLM values for accuracy)
        length_check = self.calculate_length_check(request.answer, request.maxLength)

        # Parse NCS analysis if available
        ncs_analysis = None
        if "ncs_analysis" in data and data["ncs_analysis"]:
            ncs_data = data["ncs_analysis"]
            ncs_analysis = NCSAnalysis(
                evaluated_competencies=[
                    NCSItem(
                        competency=item["competency"],
                        score=item["score"],
                        evidence=item["evidence"],
                        suggestion=item.get("suggestion"),
                    )
                    for item in ncs_data.get("evaluated_competencies", [])
                ],
                strongest=ncs_data.get("strongest", ""),
                weakest=ncs_data.get("weakest", ""),
                overall_comment=ncs_data.get("overall_comment", ""),
            )

        # Parse talent analysis if available
        talent_analysis = None
        if "talent_analysis" in data and data["talent_analysis"]:
            talent_data = data["talent_analysis"]
            talent_analysis = TalentAnalysis(
                match_score=talent_data.get("match_score", 5),
                matched_traits=talent_data.get("matched_traits", []),
                missing_traits=talent_data.get("missing_traits", []),
                overall_comment=talent_data.get("overall_comment", ""),
                improvement_tips=talent_data.get("improvement_tips", []),
            )

        return ResumeAnalysisResponse(
            overall_score=data["overall_score"],
            length_check=length_check,
            feedbacks=[
                FeedbackItem(
                    category=f["category"],
                    score=f["score"],
                    comment=f["comment"],
                    suggestion=f.get("suggestion"),
                )
                for f in data.get("feedbacks", [])
            ],
            keyword_analysis=KeywordAnalysis(
                found_keywords=data["keyword_analysis"]["found_keywords"],
                missing_keywords=data["keyword_analysis"]["missing_keywords"],
                match_rate=data["keyword_analysis"]["match_rate"],
            ),
            ncs_analysis=ncs_analysis,
            talent_analysis=talent_analysis,
            expected_questions=data.get("expected_questions", []),
        )

    def calculate_length_check(self, answer: str, max_length: int) -> LengthCheck:
        """Calculate length check for answer.

        Args:
            answer: Answer text
            max_length: Maximum allowed length

        Returns:
            Length check result
        """
        current = len(answer)
        percentage = (current / max_length) * 100 if max_length > 0 else 0

        if percentage > 100:
            status = "over"
        elif percentage >= 70:
            status = "optimal"
        else:
            status = "short"

        return LengthCheck(
            current=current,
            max=max_length,
            percentage=round(percentage, 1),
            status=status,
        )

    def check_warnings(self, answer: str, organization: str) -> list[WarningItem]:
        """ìì†Œì„œ ê³µí†µ ì‹¤ìˆ˜ ì²´í¬.

        Args:
            answer: ìì†Œì„œ ë‹µë³€ í…ìŠ¤íŠ¸
            organization: ê¸°ê´€ ì½”ë“œ

        Returns:
            ê²½ê³  í•­ëª© ëª©ë¡
        """
        warnings: list[WarningItem] = []

        # 1. ë¸”ë¼ì¸ë“œ ì±„ìš© ìœ„ë°˜ ì²´í¬
        warnings.extend(self._check_blind_violations(answer))

        # 2. ì¶”ìƒì  í‘œí˜„ ì²´í¬
        warnings.extend(self._check_abstract_expressions(answer))

        # 3. ìˆ˜ì¹˜/ê²°ê³¼ ë¶€ì¬ ì²´í¬
        warnings.extend(self._check_missing_results(answer))

        # 4. ê¸°ê´€ëª… ì˜¤ë¥˜ ì²´í¬
        warnings.extend(self._check_organization_name(answer, organization))

        return warnings

    def _check_blind_violations(self, answer: str) -> list[WarningItem]:
        """ë¸”ë¼ì¸ë“œ ì±„ìš© ìœ„ë°˜ ì²´í¬."""
        warnings: list[WarningItem] = []

        # í•™êµëª… íŒ¨í„´ (ì‚¬ì „ ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)
        for compiled_pattern, message in _SCHOOL_PATTERNS:
            matches = compiled_pattern.findall(answer)
            if matches:
                warnings.append(WarningItem(
                    type="blind_violation",
                    severity="high",
                    message=f"âš ï¸ ë¸”ë¼ì¸ë“œ ì±„ìš© ìœ„ë°˜: {message}",
                    detected_text=", ".join(set(matches)),
                    suggestion="í•™êµëª…ì„ 'ëŒ€í•™êµì—ì„œ', 'í•™ë¶€ ê³¼ì •ì—ì„œ' ë“±ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.",
                ))
                break

        # ê°€ì¡±ê´€ê³„ íŒ¨í„´ - ë¬¸ë§¥ ê¸°ë°˜ ì²´í¬
        family_warnings = self._check_family_context(answer)
        warnings.extend(family_warnings)

        # ì¶œì‹  ì§€ì—­ íŒ¨í„´ (ì‚¬ì „ ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)
        for compiled_pattern, message in _REGION_PATTERNS:
            match = compiled_pattern.search(answer)
            if match:
                warnings.append(WarningItem(
                    type="blind_violation",
                    severity="high",
                    message=f"âš ï¸ ë¸”ë¼ì¸ë“œ ì±„ìš© ìœ„ë°˜: {message}",
                    detected_text=match.group(),
                    suggestion="ì¶œì‹  ì§€ì—­ ì–¸ê¸‰ì„ ì‚­ì œí•˜ì„¸ìš”.",
                ))
                break

        return warnings

    def _check_family_context(self, answer: str) -> list[WarningItem]:
        """ê°€ì¡± ê´€ê³„ ì–¸ê¸‰ì„ ë¬¸ë§¥ ê¸°ë°˜ìœ¼ë¡œ ì²´í¬.

        ë‹¨ìˆœ ë‹¨ì–´ ë§¤ì¹­ì´ ì•„ë‹Œ, ì‹¤ì œë¡œ ê°œì¸ ê°€ì¡± ì •ë³´ë¥¼ ë…¸ì¶œí•˜ëŠ” ë§¥ë½ì¸ì§€ í™•ì¸.
        ë¬¸í™”ì /ê´€ìš©ì  í‘œí˜„ì€ ë¸”ë¼ì¸ë“œ ìœ„ë°˜ìœ¼ë¡œ ê°„ì£¼í•˜ì§€ ì•ŠìŒ.
        """
        warnings: list[WarningItem] = []

        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ì‚¬ì „ ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©)
        sentences = _SENTENCE_SPLIT_PATTERN.split(answer)

        # ê°€ì¡± ê´€ë ¨ í‚¤ì›Œë“œ (ê¸´ í‚¤ì›Œë“œë§Œ - ì§§ì€ ê²ƒì€ ì˜¤íƒ ìœ ë°œ)
        # "í˜•", "ë™ìƒ" ë“±ì€ ë‹¤ë¥¸ ë‹¨ì–´ì— í¬í•¨ë  ìˆ˜ ìˆì–´ ì œì™¸
        family_keywords_long = ["ì•„ë²„ì§€", "ì–´ë¨¸ë‹ˆ", "ë¶€ëª¨ë‹˜", "ë¶€ì¹œ", "ëª¨ì¹œ", "í• ì•„ë²„ì§€", "í• ë¨¸ë‹ˆ"]

        # ì§§ì€ ê°€ì¡± í‚¤ì›Œë“œëŠ” ì‚¬ì „ ì»´íŒŒì¼ëœ íŒ¨í„´ ì‚¬ìš©
        # _FAMILY_KEYWORDS_SHORT ëª¨ë“ˆ ë ˆë²¨ ìƒìˆ˜ ì°¸ì¡°

        # ê°œì¸ ì •ë³´ ë…¸ì¶œ ë§¥ë½, ì˜ˆì™¸ íŒ¨í„´ì€ ëª¨ë“ˆ ë ˆë²¨ ìƒìˆ˜ ì‚¬ìš©
        # _PERSONAL_CONTEXT_PATTERNS, _EXCEPTION_PATTERNS ì°¸ì¡°

        for sentence in sentences:
            if len(sentence.strip()) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì€ ìŠ¤í‚µ
                continue

            # ì˜ˆì™¸ íŒ¨í„´ì— í•´ë‹¹í•˜ë©´ ìŠ¤í‚µ (ë¬¸í™”ì /ê´€ìš©ì  í‘œí˜„) - ìºì‹œëœ íŒ¨í„´ ì‚¬ìš©
            is_exception = False
            for exc_pattern in _EXCEPTION_PATTERNS:
                if exc_pattern.search(sentence):
                    is_exception = True
                    break

            if is_exception:
                continue

            # ê°€ì¡± í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸ (ê¸´ í‚¤ì›Œë“œ)
            found_family_word = None
            for family_word in family_keywords_long:
                if family_word in sentence:
                    found_family_word = family_word
                    break

            # ì§§ì€ í‚¤ì›Œë“œëŠ” ì‚¬ì „ ì»´íŒŒì¼ëœ íŒ¨í„´ìœ¼ë¡œ ì²´í¬
            if not found_family_word:
                for compiled_pattern, word in _FAMILY_KEYWORDS_SHORT:
                    if compiled_pattern.search(sentence):
                        found_family_word = word
                        break

            if not found_family_word:
                continue

            # ê°œì¸ ì •ë³´ ë…¸ì¶œ ë§¥ë½ì¸ì§€ í™•ì¸ - ìºì‹œëœ íŒ¨í„´ ì‚¬ìš©
            for context_pattern in _PERSONAL_CONTEXT_PATTERNS:
                if context_pattern.search(sentence):
                    warnings.append(WarningItem(
                        type="blind_violation",
                        severity="high",
                        message="âš ï¸ ë¸”ë¼ì¸ë“œ ì±„ìš© ìœ„ë°˜: ê°€ì¡± ê´€ê³„/ë°°ê²½ì´ ì–¸ê¸‰ë˜ì–´ ìˆìŠµë‹ˆë‹¤",
                        detected_text=sentence.strip()[:50] + ("..." if len(sentence.strip()) > 50 else ""),
                        suggestion="ê°€ì¡±ì˜ ì§ì—…, ì˜í–¥, ë°°ê²½ ë“±ì— ëŒ€í•œ ì–¸ê¸‰ì„ ì‚­ì œí•˜ê±°ë‚˜ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ëŒ€ì²´í•˜ì„¸ìš”.",
                    ))
                    return warnings  # í•˜ë‚˜ë§Œ ë°œê²¬í•´ë„ ì¶©ë¶„

        # ì§ì ‘ì ì¸ ê°€ì •í™˜ê²½ ì–¸ê¸‰ì€ ì—¬ì „íˆ ì²´í¬ - ìºì‹œëœ íŒ¨í„´ ì‚¬ìš©
        for compiled_pattern, message in _DIRECT_FAMILY_PATTERNS:
            match = compiled_pattern.search(answer)
            if match:
                warnings.append(WarningItem(
                    type="blind_violation",
                    severity="high",
                    message=f"âš ï¸ ë¸”ë¼ì¸ë“œ ì±„ìš© ìœ„ë°˜: {message}",
                    detected_text=match.group(),
                    suggestion="ê°€ì •í™˜ê²½ ê´€ë ¨ ì–¸ê¸‰ì„ ì‚­ì œí•˜ì„¸ìš”.",
                ))
                break

        return warnings

    def _check_abstract_expressions(self, answer: str) -> list[WarningItem]:
        """ì¶”ìƒì  í‘œí˜„ ì²´í¬."""
        warnings: list[WarningItem] = []

        abstract_patterns = [
            ("ì—´ì‹¬íˆ í•˜ê² ìŠµë‹ˆë‹¤", "êµ¬ì²´ì ì¸ ê³„íšì´ë‚˜ ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”"),
            ("ìµœì„ ì„ ë‹¤í•˜ê² ìŠµë‹ˆë‹¤", "ì–´ë–»ê²Œ ìµœì„ ì„ ë‹¤í•  ê²ƒì¸ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”"),
            ("ì—´ì •ì ìœ¼ë¡œ", "ì—´ì •ì˜ êµ¬ì²´ì ì¸ ì¦ê±°ë‚˜ ê²½í—˜ì„ ì œì‹œí•˜ì„¸ìš”"),
            ("ì„±ì‹¤í•˜ê²Œ", "ì„±ì‹¤í•¨ì„ ë³´ì—¬ì£¼ëŠ” êµ¬ì²´ì ì¸ ê²½í—˜ì„ ì¶”ê°€í•˜ì„¸ìš”"),
            ("ë…¸ë ¥í•˜ê² ìŠµë‹ˆë‹¤", "êµ¬ì²´ì ì¸ ì‹¤ì²œ ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”"),
            ("ê·€ì‚¬ì˜ ë°œì „ì— ê¸°ì—¬", "ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ê¸°ì—¬í•  ê²ƒì¸ì§€ êµ¬ì²´í™”í•˜ì„¸ìš”"),
            ("ë‹¤ì–‘í•œ ê²½í—˜", "'ë‹¤ì–‘í•œ'ì„ êµ¬ì²´ì ì¸ ê²½í—˜ ë‚˜ì—´ë¡œ ëŒ€ì²´í•˜ì„¸ìš”"),
            ("ë§ì€ ê²ƒì„ ë°°ì› ìŠµë‹ˆë‹¤", "ë¬´ì—‡ì„ ë°°ì› ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”"),
        ]

        found_abstracts = []
        for phrase, suggestion in abstract_patterns:
            if phrase in answer:
                found_abstracts.append((phrase, suggestion))

        if found_abstracts:
            # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ê²½ê³ 
            for phrase, suggestion in found_abstracts[:3]:
                warnings.append(WarningItem(
                    type="abstract_expression",
                    severity="medium",
                    message=f"ğŸ’­ ì¶”ìƒì  í‘œí˜„: \"{phrase}\"",
                    detected_text=phrase,
                    suggestion=suggestion,
                ))

        return warnings

    def _check_missing_results(self, answer: str) -> list[WarningItem]:
        """ìˆ˜ì¹˜/ê²°ê³¼ ë¶€ì¬ ì²´í¬."""
        warnings: list[WarningItem] = []

        # ìˆ˜ì¹˜ê°€ ìˆëŠ”ì§€ í™•ì¸ (%, ë°°, ëª…, ê±´, ì›, ê°œì›” ë“±) - ìºì‹œëœ íŒ¨í„´ ì‚¬ìš©
        has_numbers = bool(_NUMBER_PATTERN.search(answer))

        # ê²°ê³¼ í‘œí˜„ì´ ìˆëŠ”ì§€ í™•ì¸ - ìºì‹œëœ íŒ¨í„´ ì‚¬ìš©
        has_result = any(p.search(answer) for p in _RESULT_PATTERNS)

        if not has_numbers and not has_result:
            warnings.append(WarningItem(
                type="missing_result",
                severity="medium",
                message="ğŸ“Š êµ¬ì²´ì ì¸ ì„±ê³¼/ìˆ˜ì¹˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
                detected_text="",
                suggestion="ê²½í—˜ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•´ ë³´ì„¸ìš”. ì˜ˆ: 'ì „ë…„ ëŒ€ë¹„ 20% í–¥ìƒ', '3ê°œì›” ë§Œì— ì™„ë£Œ'",
            ))

        return warnings

    def _check_organization_name(self, answer: str, organization: str) -> list[WarningItem]:
        """ê¸°ê´€ëª… ì˜¤ë¥˜ ì²´í¬."""
        warnings: list[WarningItem] = []

        # ë‹¤ë¥¸ ê¸°ê´€ëª…ì´ ë“¤ì–´ìˆëŠ”ì§€ í™•ì¸ - ìºì‹œëœ íŒ¨í„´ ì‚¬ìš©
        for compiled_pattern, org_code in _ORG_NAME_PATTERNS:
            if org_code != organization.upper():
                match = compiled_pattern.search(answer)
                if match:
                    warnings.append(WarningItem(
                        type="wrong_organization",
                        severity="high",
                        message=f"ğŸš¨ ë‹¤ë¥¸ ê¸°ê´€ëª… ë°œê²¬: \"{match.group()}\"",
                        detected_text=match.group(),
                        suggestion="ì§€ì› ê¸°ê´€ëª…ì´ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ë³µì‚¬-ë¶™ì—¬ë„£ê¸° ì˜¤ë¥˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    ))
                    break

        return warnings

    def analyze_core_values(
        self, answer: str, core_values: list[str]
    ) -> list[CoreValueScore]:
        """í•µì‹¬ê°€ì¹˜ë³„ ë°˜ì˜ ì ìˆ˜ ë¶„ì„.

        Args:
            answer: ìì†Œì„œ ë‹µë³€ í…ìŠ¤íŠ¸
            core_values: ê¸°ê´€ì˜ í•µì‹¬ê°€ì¹˜ ëª©ë¡

        Returns:
            í•µì‹¬ê°€ì¹˜ë³„ ì ìˆ˜ ëª©ë¡
        """
        scores: list[CoreValueScore] = []

        # í•µì‹¬ê°€ì¹˜ë³„ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
        value_keywords: dict[str, list[str]] = {
            # ê³µí†µ í•µì‹¬ê°€ì¹˜
            "ê³ ê°ì¤‘ì‹¬": ["ê³ ê°", "ìˆ˜ìš”ì", "êµ­ë¯¼", "ì„œë¹„ìŠ¤", "ë§Œì¡±", "í¸ì˜", "ì‚¬ìš©ì", "ì´ìš©ì"],
            "ê³ ê°ë§Œì¡±": ["ê³ ê°", "ë§Œì¡±", "ì„œë¹„ìŠ¤", "í¸ì˜", "ì´ìš©ì", "ìˆ˜ìš”ì"],
            "ìƒìƒ": ["ìƒìƒ", "í˜‘ë ¥", "í˜‘ì—…", "íŒŒíŠ¸ë„ˆ", "ë™ë°˜ì„±ì¥", "í•¨ê»˜", "ê³µìƒ", "ìƒí˜¸"],
            "í˜‘ë ¥": ["í˜‘ë ¥", "í˜‘ì—…", "íŒ€ì›Œí¬", "ì†Œí†µ", "í•¨ê»˜", "ê³µë™", "íŒŒíŠ¸ë„ˆì‹­"],
            "í˜ì‹ ": ["í˜ì‹ ", "ë³€í™”", "ì°½ì˜", "ê°œì„ ", "ìƒˆë¡œìš´", "ë„ì „", "ë°œì „", "ë””ì§€í„¸", "AI", "ìŠ¤ë§ˆíŠ¸"],
            "ì°½ì˜": ["ì°½ì˜", "ì°½ì¡°", "ìƒˆë¡œìš´", "ì•„ì´ë””ì–´", "í˜ì‹ ", "ë…ì°½"],
            "ì „ë¬¸ì„±": ["ì „ë¬¸", "ì—­ëŸ‰", "ì§€ì‹", "ê¸°ìˆ ", "ìˆ™ë ¨", "ê²½í—˜", "ë…¸í•˜ìš°", "ì „ë¬¸ê°€"],
            "ì²­ë ´": ["ì²­ë ´", "ìœ¤ë¦¬", "íˆ¬ëª…", "ê³µì •", "ì •ì§", "ì‹ ë¢°", "ë„ë•", "ì¤€ë²•"],
            "ì •ì§": ["ì •ì§", "ì§„ì‹¤", "ì„±ì‹¤", "ì‹ ë¢°", "íˆ¬ëª…"],
            "ì‹ ë¢°": ["ì‹ ë¢°", "ë¯¿ìŒ", "ì •ì§", "ì„±ì‹¤", "ì±…ì„"],
            "ì±…ì„": ["ì±…ì„", "ì˜ë¬´", "ì‚¬ëª…", "ì—­í• ", "ë‹´ë‹¹", "ë§¡ì€"],
            "ì•ˆì „": ["ì•ˆì „", "ë³´ì•ˆ", "ì˜ˆë°©", "ì‚¬ê³ ", "ë¦¬ìŠ¤í¬", "ìœ„í—˜ê´€ë¦¬"],
            "ì†Œí†µ": ["ì†Œí†µ", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ëŒ€í™”", "ê³µìœ ", "í˜‘ì˜", "ë…¼ì˜"],
            "ë„ì „": ["ë„ì „", "ì‹œë„", "ê·¹ë³µ", "ëª©í‘œ", "ì„±ì¥", "ë°œì „"],
            "ì—´ì •": ["ì—´ì •", "ì—´ì‹¬", "í—Œì‹ ", "ëª°ì…", "ì ê·¹", "ì˜ì§€"],
            "ì„±ì¥": ["ì„±ì¥", "ë°œì „", "í–¥ìƒ", "ì§„ë³´", "ê°œë°œ", "í•™ìŠµ"],
            "ë´‰ì‚¬": ["ë´‰ì‚¬", "ê¸°ì—¬", "ê³µí—Œ", "ë‚˜ëˆ”", "ì‚¬íšŒì "],
            "ê³µìµ": ["ê³µìµ", "ê³µê³µ", "êµ­ë¯¼", "ì‚¬íšŒ", "ê¸°ì—¬"],
            "ì¡´ì¤‘": ["ì¡´ì¤‘", "ë°°ë ¤", "ì´í•´", "í¬ìš©", "ë‹¤ì–‘ì„±"],
            "íš¨ìœ¨": ["íš¨ìœ¨", "ìƒì‚°ì„±", "ìµœì í™”", "ì ˆê°", "ê°œì„ "],
        }

        answer_lower = answer.lower()

        for value in core_values:
            # í•µì‹¬ê°€ì¹˜ ì´ë¦„ ì •ê·œí™”
            value_normalized = value.strip()
            keywords = value_keywords.get(value_normalized, [value_normalized])

            # í‚¤ì›Œë“œ ê²€ìƒ‰
            found_keywords: list[str] = []
            evidence_parts: list[str] = []

            for keyword in keywords:
                if keyword.lower() in answer_lower:
                    found_keywords.append(keyword)
                    # í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
                    sentences = _SENTENCE_SPLIT_PATTERN.split(answer)
                    for sentence in sentences:
                        if keyword in sentence and len(sentence) > 10:
                            if sentence not in evidence_parts:
                                evidence_parts.append(sentence.strip())
                            break

            # ì ìˆ˜ ê³„ì‚°
            if len(found_keywords) >= 3:
                score = 9
            elif len(found_keywords) >= 2:
                score = 7
            elif len(found_keywords) >= 1:
                score = 5
            else:
                score = 3

            # ì§ì ‘ ì–¸ê¸‰ ë³´ë„ˆìŠ¤
            if value_normalized in answer:
                score = min(10, score + 1)

            found = len(found_keywords) > 0

            # ê·¼ê±° ë˜ëŠ” ì œì•ˆ ìƒì„±
            evidence = ""
            suggestion = ""

            if found and evidence_parts:
                evidence = evidence_parts[0][:100] + ("..." if len(evidence_parts[0]) > 100 else "")
            elif not found:
                suggestion = f"'{value_normalized}' ê´€ë ¨ ê²½í—˜ì´ë‚˜ ê°€ì¹˜ê´€ì„ ì¶”ê°€í•´ ë³´ì„¸ìš”."

            scores.append(CoreValueScore(
                value=value_normalized,
                score=score,
                found=found,
                evidence=evidence,
                suggestion=suggestion,
            ))

        return scores

    def analyze_ncs_competencies(
        self, answer: str, position: str
    ) -> list[NCSCompetencyScore]:
        """NCS ì—­ëŸ‰ë³„ ë°˜ì˜ ì ìˆ˜ ë¶„ì„.

        Args:
            answer: ìì†Œì„œ ë‹µë³€ í…ìŠ¤íŠ¸
            position: ì§ë¬´ëª…

        Returns:
            NCS ì—­ëŸ‰ë³„ ì ìˆ˜ ëª©ë¡
        """
        if not self._position_manager:
            return []

        # ì§ë¬´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        position_data = self._position_manager.get_position(position)
        if not position_data:
            return []

        ncs_competencies = position_data.get("ncs_competencies", [])
        if not ncs_competencies:
            return []

        scores: list[NCSCompetencyScore] = []

        # NCS ì—­ëŸ‰ë³„ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
        ncs_keywords: dict[str, list[str]] = {
            "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥": ["ì˜ì‚¬ì†Œí†µ", "ì†Œí†µ", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ë°œí‘œ", "ë³´ê³ ", "ë¬¸ì„œì‘ì„±", "ê²½ì²­", "ì„¤ë“", "í˜‘ì˜", "ì „ë‹¬"],
            "ìˆ˜ë¦¬ëŠ¥ë ¥": ["ìˆ˜ë¦¬", "ê³„ì‚°", "í†µê³„", "ë°ì´í„°", "ë¶„ì„", "ìˆ˜ì¹˜", "ì •ëŸ‰", "ê·¸ë˜í”„", "ì—‘ì…€", "ìˆ«ì"],
            "ë¬¸ì œí•´ê²°ëŠ¥ë ¥": ["ë¬¸ì œí•´ê²°", "í•´ê²°", "ë¶„ì„", "ì›ì¸", "ëŒ€ì•ˆ", "ê°œì„ ", "ê·¹ë³µ", "ëŒ€ì²˜", "ì°½ì˜", "í˜ì‹ "],
            "ìê¸°ê°œë°œëŠ¥ë ¥": ["ìê¸°ê°œë°œ", "í•™ìŠµ", "ì„±ì¥", "ì—­ëŸ‰", "ë…¸ë ¥", "ê³µë¶€", "ìê²©ì¦", "êµìœ¡", "í›ˆë ¨", "ë°œì „"],
            "ìì›ê´€ë¦¬ëŠ¥ë ¥": ["ìì›ê´€ë¦¬", "ì˜ˆì‚°", "ì¼ì •", "ì‹œê°„ê´€ë¦¬", "ì¸ë ¥", "ë¬¼ì", "íš¨ìœ¨", "ê´€ë¦¬", "ë°°ë¶„", "ì¡°ë‹¬"],
            "ëŒ€ì¸ê´€ê³„ëŠ¥ë ¥": ["ëŒ€ì¸ê´€ê³„", "í˜‘ë ¥", "íŒ€ì›Œí¬", "ê°ˆë“±", "ì¡°ìœ¨", "ë¦¬ë”ì‹­", "íŒ”ë¡œì›Œì‹­", "ë„¤íŠ¸ì›Œí¬", "ê´€ê³„", "ì¡°ì •"],
            "ì •ë³´ëŠ¥ë ¥": ["ì •ë³´", "ë°ì´í„°", "ì‹œìŠ¤í…œ", "ì»´í“¨í„°", "IT", "ë¶„ì„", "ìˆ˜ì§‘", "í™œìš©", "ë””ì§€í„¸", "DB"],
            "ê¸°ìˆ ëŠ¥ë ¥": ["ê¸°ìˆ ", "ì „ë¬¸", "ìˆ™ë ¨", "ë„êµ¬", "ì¥ë¹„", "ì‹œìŠ¤í…œ", "ê°œë°œ", "ì„¤ê³„", "êµ¬í˜„", "ìš´ì˜"],
            "ì¡°ì§ì´í•´ëŠ¥ë ¥": ["ì¡°ì§", "ê¸°ê´€", "íšŒì‚¬", "ë¬¸í™”", "ë¹„ì „", "ë¯¸ì…˜", "ì „ëµ", "ëª©í‘œ", "ë¶€ì„œ", "ì—…ë¬´"],
            "ì§ì—…ìœ¤ë¦¬": ["ìœ¤ë¦¬", "ì²­ë ´", "ì±…ì„", "ì„±ì‹¤", "ì •ì§", "ì¤€ë²•", "ê³µì •", "ë„ë•", "ì‹ ë¢°", "ìœ¤ë¦¬ì˜ì‹"],
        }

        answer_lower = answer.lower()

        for ncs in ncs_competencies:
            ncs_name = ncs.get("name", "")
            ncs_code = ncs.get("code", "")
            importance = ncs.get("importance", "ê¶Œì¥")

            keywords = ncs_keywords.get(ncs_name, [ncs_name])

            # í‚¤ì›Œë“œ ê²€ìƒ‰
            found_keywords: list[str] = []
            evidence_parts: list[str] = []

            for keyword in keywords:
                if keyword.lower() in answer_lower or keyword in answer:
                    found_keywords.append(keyword)
                    # í•´ë‹¹ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
                    sentences = _SENTENCE_SPLIT_PATTERN.split(answer)
                    for sentence in sentences:
                        if keyword in sentence and len(sentence) > 10:
                            if sentence not in evidence_parts:
                                evidence_parts.append(sentence.strip())
                            break

            # ì ìˆ˜ ê³„ì‚° (ì¤‘ìš”ë„ì— ë”°ë¼ ê°€ì¤‘ì¹˜)
            base_score = 3
            if len(found_keywords) >= 3:
                base_score = 9
            elif len(found_keywords) >= 2:
                base_score = 7
            elif len(found_keywords) >= 1:
                base_score = 5

            # ì¤‘ìš”ë„ê°€ í•„ìˆ˜ì¸ë° ë°œê²¬ë˜ì§€ ì•Šìœ¼ë©´ ê°ì 
            if importance == "í•„ìˆ˜" and len(found_keywords) == 0:
                base_score = 2

            found = len(found_keywords) > 0

            # ê·¼ê±° ë˜ëŠ” ì œì•ˆ ìƒì„±
            evidence = ""
            suggestion = ""

            # NCS ì—­ëŸ‰ë³„ êµ¬ì²´ì  ì œì•ˆ ë©”ì‹œì§€
            ncs_suggestions: dict[str, str] = {
                "ì˜ì‚¬ì†Œí†µëŠ¥ë ¥": "íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ì˜ê²¬ ì¡°ìœ¨, ë°œí‘œ ê²½í—˜, ë³´ê³ ì„œ ì‘ì„± ë“± ì†Œí†µ ì—­ëŸ‰ì„ ë³´ì—¬ì£¼ëŠ” êµ¬ì²´ì  ì‚¬ë¡€ë¥¼ ì¶”ê°€í•˜ì„¸ìš”. ì˜ˆ: 'ì£¼ê°„ íšŒì˜ì—ì„œ ì§„í–‰ ìƒí™©ì„ ë³´ê³ í•˜ê³ , íŒ€ì›ë“¤ì˜ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬...'",
                "ìˆ˜ë¦¬ëŠ¥ë ¥": "ë°ì´í„° ë¶„ì„, í†µê³„ í™œìš©, ìˆ˜ì¹˜ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•˜ì„¸ìš”. ì˜ˆ: 'ë§¤ì¶œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì „ë…„ ëŒ€ë¹„ 15% ì„±ì¥ ì›ì¸ì„ íŒŒì•…í•˜ê³ ...'",
                "ë¬¸ì œí•´ê²°ëŠ¥ë ¥": "ì–´ë ¤ìš´ ìƒí™©ì„ ê·¹ë³µí•œ ê²½í—˜ì„ ìƒí™©-ë¬¸ì œ-í•´ê²°-ê²°ê³¼(STAR) êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì˜ˆ: 'í”„ë¡œì íŠ¸ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ê¸°ìˆ ì  ë¬¸ì œê°€ ë°œìƒí–ˆì„ ë•Œ, ëŒ€ì•ˆì„ ë¶„ì„í•˜ê³ ...'",
                "ìê¸°ê°œë°œëŠ¥ë ¥": "ìê²©ì¦ ì·¨ë“, êµìœ¡ ì´ìˆ˜, ìê¸°ì£¼ë„ í•™ìŠµ ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•˜ì„¸ìš”. ì˜ˆ: 'ì—…ë¬´ ì—­ëŸ‰ í–¥ìƒì„ ìœ„í•´ Python ì˜¨ë¼ì¸ ê°•ì¢Œë¥¼ ìˆ˜ë£Œí•˜ê³ , ì‹¤ë¬´ì— ì ìš©í•˜ì—¬...'",
                "ìì›ê´€ë¦¬ëŠ¥ë ¥": "ì˜ˆì‚° ê´€ë¦¬, ì¼ì • ì¡°ìœ¨, ì¸ë ¥ ë°°ë¶„ ë“± ìì›ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•œ ê²½í—˜ì„ ì¶”ê°€í•˜ì„¸ìš”. ì˜ˆ: 'í•œì •ëœ ì˜ˆì‚° ë‚´ì—ì„œ ìš°ì„ ìˆœìœ„ë¥¼ ì„¤ì •í•˜ì—¬ í•µì‹¬ ê³¼ì œì— ì§‘ì¤‘í•œ ê²°ê³¼...'",
                "ëŒ€ì¸ê´€ê³„ëŠ¥ë ¥": "íŒ€ì›Œí¬, ê°ˆë“± í•´ê²°, í˜‘ë ¥ ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì˜ˆ: 'íŒ€ì› ê°„ ì˜ê²¬ ì¶©ëŒ ì‹œ ê°ìì˜ ì…ì¥ì„ ê²½ì²­í•˜ê³  ì¤‘ì¬í•˜ì—¬ í•©ì˜ì ì„ ë„ì¶œí–ˆìŠµë‹ˆë‹¤...'",
                "ì •ë³´ëŠ¥ë ¥": "ì •ë³´ ìˆ˜ì§‘, ë°ì´í„° í™œìš©, IT ì‹œìŠ¤í…œ í™œìš© ê²½í—˜ì„ ì¶”ê°€í•˜ì„¸ìš”. ì˜ˆ: 'ì—…ë¬´ íš¨ìœ¨í™”ë¥¼ ìœ„í•´ ì—‘ì…€ ë§¤í¬ë¡œë¥¼ í™œìš©í•˜ì—¬ ë°˜ë³µ ì‘ì—…ì„ ìë™í™”í•˜ê³ ...'",
                "ê¸°ìˆ ëŠ¥ë ¥": "ì „ë¬¸ ê¸°ìˆ , ë„êµ¬ í™œìš©, ì‹œìŠ¤í…œ ìš´ì˜ ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•˜ì„¸ìš”. ì˜ˆ: 'AutoCADë¥¼ í™œìš©í•œ ë„ë©´ ì‘ì„±, Python ë°ì´í„° ë¶„ì„ ë“± ì‹¤ë¬´ ê¸°ìˆ ì„ ì ìš©í•˜ì—¬...'",
                "ì¡°ì§ì´í•´ëŠ¥ë ¥": "ì§€ì› ê¸°ê´€ì˜ ë¯¸ì…˜, ë¹„ì „, ìµœê·¼ ì‚¬ì—…ì— ëŒ€í•œ ì´í•´ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”. ì˜ˆ: 'ê¸°ê´€ì˜ ë””ì§€í„¸ ì „í™˜ ì „ëµì— ê³µê°í•˜ë©°, ì œ IT ì—­ëŸ‰ì„ í™œìš©í•˜ì—¬ ê¸°ì—¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤...'",
                "ì§ì—…ìœ¤ë¦¬": "ì±…ì„ê°, ì„±ì‹¤ì„±, ì²­ë ´ì„±ì„ ë³´ì—¬ì£¼ëŠ” ê²½í—˜ì„ ì¶”ê°€í•˜ì„¸ìš”. ì˜ˆ: 'ë§¡ì€ ì—…ë¬´ì— ëŒ€í•œ ì±…ì„ê°ìœ¼ë¡œ ì•¼ê·¼ë„ ë§ˆë‹¤í•˜ì§€ ì•Šê³  ê¸°í•œ ë‚´ ì™„ìˆ˜í–ˆìŠµë‹ˆë‹¤...'",
            }

            if found and evidence_parts:
                evidence = evidence_parts[0][:100] + ("..." if len(evidence_parts[0]) > 100 else "")
            elif not found:
                specific_suggestion = ncs_suggestions.get(ncs_name, f"'{ncs_name}' ê´€ë ¨ êµ¬ì²´ì ì¸ ê²½í—˜ì„ ì¶”ê°€í•˜ì„¸ìš”.")
                if importance == "í•„ìˆ˜":
                    suggestion = f"[í•„ìˆ˜ ì—­ëŸ‰] {specific_suggestion}"
                else:
                    suggestion = specific_suggestion

            scores.append(NCSCompetencyScore(
                code=ncs_code,
                name=ncs_name,
                importance=importance,
                score=base_score,
                found=found,
                evidence=evidence,
                suggestion=suggestion,
            ))

        return scores

    def find_similar_questions(
        self, question: str | None, organization: str
    ) -> list[SimilarQuestion]:
        """í˜„ì¬ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ê¸°ì¶œë¬¸í•­ ì°¾ê¸°.

        Args:
            question: í˜„ì¬ ì§ˆë¬¸ (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
            organization: ê¸°ê´€ ì½”ë“œ

        Returns:
            ìœ ì‚¬ ê¸°ì¶œë¬¸í•­ ëª©ë¡ (ìœ ì‚¬ë„ ìˆœ)
        """
        if not question:
            return []

        from pathlib import Path

        similar: list[SimilarQuestion] = []
        knowledge_db_path = Path(__file__).parent.parent.parent.parent / "resume-knowledge-db" / "data" / "questions"

        if not knowledge_db_path.exists():
            return []

        # í˜„ì¬ ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_keywords = self._extract_question_keywords(question)

        # ìµœê·¼ 5ë…„ê°„ì˜ ê¸°ì¶œë¬¸í•­ í™•ì¸
        for year in [2025, 2024, 2023, 2022, 2021]:
            question_file = knowledge_db_path / f"{organization.upper()}_{year}.json"
            if not question_file.exists():
                continue

            try:
                import json
                with open(question_file, encoding="utf-8") as f:
                    data = json.load(f)

                metadata = data.get("metadata", {})
                # ì˜ˆìƒ ë¬¸í•­(verified=False)ì€ ìœ ì‚¬ë„ ë¶„ì„ì—ì„œ ì œì™¸
                if not metadata.get("verified", True):
                    continue

                half = data.get("half", "")

                for q in data.get("questions", []):
                    q_text = q.get("text", "")
                    q_keywords = q.get("keywords", [])
                    q_ncs = q.get("ncs_categories", [])

                    # ìœ ì‚¬ë„ ê³„ì‚°
                    similarity, matched = self._calculate_similarity(
                        question_keywords, q_text, q_keywords, q_ncs
                    )

                    if similarity >= 30:  # 30% ì´ìƒë§Œ í¬í•¨
                        similar.append(SimilarQuestion(
                            year=year,
                            half=half,
                            question=q_text,
                            similarity=similarity,
                            char_limit=q.get("char_limit", 0),
                            matched_keywords=matched,
                        ))

            except Exception:
                continue

        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬, ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
        similar.sort(key=lambda x: x.similarity, reverse=True)
        return similar[:5]

    def _extract_question_keywords(self, question: str) -> set[str]:
        """ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ.

        Args:
            question: ì§ˆë¬¸ í…ìŠ¤íŠ¸

        Returns:
            í‚¤ì›Œë“œ ì§‘í•©
        """
        # ì£¼ìš” í‚¤ì›Œë“œ íŒ¨í„´
        keyword_patterns = [
            # ì§ˆë¬¸ ìœ í˜•
            "ì§€ì›ë™ê¸°", "ì…ì‚¬ í›„", "ì¥ë‹¨ì ", "ê°•ì ", "ì•½ì ", "ê²½í—˜", "ì‚¬ë¡€",
            "ê°ˆë“±", "í˜‘ë ¥", "íŒ€ì›Œí¬", "ë¦¬ë”ì‹­", "ë¬¸ì œí•´ê²°", "ì„±ê³¼", "ì‹¤íŒ¨",
            "ê·¹ë³µ", "ë„ì „", "ëª©í‘œ", "ê³„íš", "ê°€ì¹˜ê´€", "ì¸ì¬ìƒ", "ì§ë¬´",
            # NCS ì—­ëŸ‰
            "ì˜ì‚¬ì†Œí†µ", "ì†Œí†µ", "ë¬¸ì œí•´ê²°", "ìê¸°ê°œë°œ", "ëŒ€ì¸ê´€ê³„", "ê°ˆë“±ê´€ë¦¬",
            "ì¡°ì§ì´í•´", "ê¸°ìˆ ", "ì •ë³´", "ìˆ˜ë¦¬", "ìœ¤ë¦¬", "ì±…ì„",
            # ìƒí™©/ê²½í—˜
            "íŒ€", "í”„ë¡œì íŠ¸", "ì—…ë¬´", "ì„±ê³µ", "ì‹¤íŒ¨", "ì–´ë ¤ì›€", "ë…¸ë ¥",
            "ê²°ê³¼", "ì„±ì¥", "í•™ìŠµ", "ê°œì„ ", "í˜ì‹ ",
        ]

        keywords = set()
        question_lower = question.lower()

        for kw in keyword_patterns:
            if kw in question or kw.lower() in question_lower:
                keywords.add(kw)

        return keywords

    def _calculate_similarity(
        self,
        question_keywords: set[str],
        past_question: str,
        past_keywords: list[str],
        past_ncs: list[str],
    ) -> tuple[float, list[str]]:
        """ìœ ì‚¬ë„ ê³„ì‚°.

        Args:
            question_keywords: í˜„ì¬ ì§ˆë¬¸ í‚¤ì›Œë“œ
            past_question: ê³¼ê±° ì§ˆë¬¸ í…ìŠ¤íŠ¸
            past_keywords: ê³¼ê±° ì§ˆë¬¸ í‚¤ì›Œë“œ
            past_ncs: ê³¼ê±° ì§ˆë¬¸ NCS ì¹´í…Œê³ ë¦¬

        Returns:
            (ìœ ì‚¬ë„, ì¼ì¹˜ í‚¤ì›Œë“œ ëª©ë¡)
        """
        matched: list[str] = []
        total_weight = 0
        matched_weight = 0

        # í‚¤ì›Œë“œ ì¼ì¹˜ í™•ì¸
        all_past_keywords = set(past_keywords + past_ncs)

        for kw in question_keywords:
            total_weight += 1
            if kw in past_question or kw.lower() in past_question.lower():
                matched.append(kw)
                matched_weight += 1
            elif kw in all_past_keywords:
                matched.append(kw)
                matched_weight += 0.8  # í‚¤ì›Œë“œ ëª©ë¡ì—ë§Œ ìˆìœ¼ë©´ 80%

        # ê³¼ê±° í‚¤ì›Œë“œê°€ í˜„ì¬ ì§ˆë¬¸ì— ìˆëŠ”ì§€ë„ í™•ì¸
        for kw in all_past_keywords:
            if kw not in question_keywords and kw in str(question_keywords):
                # ì´ë¯¸ ì¹´ìš´íŠ¸ë˜ì§€ ì•Šì€ ê²½ìš°
                continue

        if total_weight == 0:
            return 0, matched

        similarity = (matched_weight / max(total_weight, len(all_past_keywords) / 2)) * 100
        return min(similarity, 100), matched

    def analyze_position_skills(
        self, answer: str, position: str
    ) -> PositionSkillMatch:
        """ì§ë¬´ë³„ ìš°ëŒ€ì‚¬í•­ ë§¤ì¹­ ë¶„ì„.

        Args:
            answer: ìì†Œì„œ ë‹µë³€ í…ìŠ¤íŠ¸
            position: ì§ë¬´ëª…

        Returns:
            ì§ë¬´ë³„ ìŠ¤í‚¬ ë§¤ì¹­ ê²°ê³¼
        """
        if not self._position_manager:
            return PositionSkillMatch()

        # ì§ë¬´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        position_data = self._position_manager.get_position(position)
        if not position_data:
            return PositionSkillMatch()

        requirements = position_data.get("common_requirements", {})
        if not requirements:
            return PositionSkillMatch()

        answer_lower = answer.lower()

        # ì „ê³µ ë§¤ì¹­
        majors = requirements.get("majors", [])
        matched_majors: list[str] = []
        missing_majors: list[str] = []

        # ì „ê³µ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘ (ì „ê³µëª… + ê´€ë ¨ ë‹¨ì–´)
        major_keywords: dict[str, list[str]] = {
            "ê²½ì˜í•™": ["ê²½ì˜", "ê²½ì˜í•™", "ë§ˆì¼€íŒ…", "ì¸ì‚¬", "ì¡°ì§", "ì¬ë¬´", "íšŒê³„"],
            "ê²½ì œí•™": ["ê²½ì œ", "ê²½ì œí•™", "ê±°ì‹œ", "ë¯¸ì‹œ", "ê¸ˆìœµ"],
            "í–‰ì •í•™": ["í–‰ì •", "í–‰ì •í•™", "ê³µê³µ", "ì •ì±…"],
            "ì‚°ì—…ê³µí•™": ["ì‚°ì—…ê³µí•™", "IE", "í’ˆì§ˆ", "ìƒì‚°", "ë¬¼ë¥˜", "SCM"],
            "í†µê³„í•™": ["í†µê³„", "í†µê³„í•™", "ë°ì´í„°", "ë¶„ì„", "í™•ë¥ "],
            "ì»´í“¨í„°ê³µí•™": ["ì»´í“¨í„°", "IT", "ê°œë°œ", "í”„ë¡œê·¸ë˜ë°", "ì†Œí”„íŠ¸ì›¨ì–´", "ì½”ë”©"],
            "ì „ìê³µí•™": ["ì „ì", "ì „ìê³µí•™", "íšŒë¡œ", "ë°˜ë„ì²´", "ì„ë² ë””ë“œ"],
            "ì „ê¸°ê³µí•™": ["ì „ê¸°", "ì „ê¸°ê³µí•™", "ì „ë ¥", "ë°°ì „", "ì†¡ì „"],
            "ê¸°ê³„ê³µí•™": ["ê¸°ê³„", "ê¸°ê³„ê³µí•™", "ì„¤ê³„", "CAD", "ì œì¡°"],
            "í™”í•™ê³µí•™": ["í™”í•™", "í™”í•™ê³µí•™", "ê³µì •", "í”ŒëœíŠ¸"],
            "í† ëª©ê³µí•™": ["í† ëª©", "í† ëª©ê³µí•™", "ê±´ì„¤", "êµ¬ì¡°", "ì‹œê³µ"],
            "ê±´ì¶•ê³µí•™": ["ê±´ì¶•", "ê±´ì¶•ê³µí•™", "ì„¤ê³„", "ì‹œê³µ"],
            "í™˜ê²½ê³µí•™": ["í™˜ê²½", "í™˜ê²½ê³µí•™", "íìˆ˜", "ëŒ€ê¸°", "íê¸°ë¬¼"],
            "íšŒê³„í•™": ["íšŒê³„", "íšŒê³„í•™", "ì¬ë¬´", "ì„¸ë¬´", "ê°ì‚¬"],
            "ë²•í•™": ["ë²•", "ë²•í•™", "ë²•ë¥ ", "ê³„ì•½", "ì†Œì†¡"],
            "ì‹¬ë¦¬í•™": ["ì‹¬ë¦¬", "ì‹¬ë¦¬í•™", "ìƒë‹´", "ì¸ì‚¬"],
            "ì‚¬íšŒí•™": ["ì‚¬íšŒ", "ì‚¬íšŒí•™", "ì¡°ì‚¬", "í†µê³„"],
        }

        for major in majors:
            keywords = major_keywords.get(major, [major])
            found = False
            for kw in keywords:
                if kw.lower() in answer_lower or kw in answer:
                    matched_majors.append(major)
                    found = True
                    break
            if not found:
                missing_majors.append(major)

        # ìê²©ì¦ ë§¤ì¹­
        certifications = requirements.get("certifications", [])
        matched_certs: list[str] = []
        missing_certs: list[str] = []

        # ìê²©ì¦ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
        cert_keywords: dict[str, list[str]] = {
            "ê²½ì˜ì§€ë„ì‚¬": ["ê²½ì˜ì§€ë„ì‚¬"],
            "ê³µì¸íšŒê³„ì‚¬(CPA)": ["CPA", "ê³µì¸íšŒê³„ì‚¬", "íšŒê³„ì‚¬"],
            "íˆ¬ììì‚°ìš´ìš©ì‚¬": ["íˆ¬ììì‚°ìš´ìš©", "í€ë“œë§¤ë‹ˆì €"],
            "ì¬ê²½ê´€ë¦¬ì‚¬": ["ì¬ê²½ê´€ë¦¬ì‚¬", "ì¬ê²½"],
            "ì»´í“¨í„°í™œìš©ëŠ¥ë ¥": ["ì»´í“¨í„°í™œìš©ëŠ¥ë ¥", "ì»´í™œ", "ì—‘ì…€", "Excel"],
            "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬": ["ì •ë³´ì²˜ë¦¬ê¸°ì‚¬", "ì •ì²˜ê¸°"],
            "ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬": ["ë¹…ë°ì´í„°", "ë°ì´í„°ë¶„ì„"],
            "SQLD": ["SQLD", "SQL", "ë°ì´í„°ë² ì´ìŠ¤"],
            "PMP": ["PMP", "í”„ë¡œì íŠ¸ê´€ë¦¬"],
            "ì „ê¸°ê¸°ì‚¬": ["ì „ê¸°ê¸°ì‚¬", "ì „ê¸°"],
            "ì „ê¸°ê³µì‚¬ê¸°ì‚¬": ["ì „ê¸°ê³µì‚¬ê¸°ì‚¬"],
            "ì „ê¸°ì‚°ì—…ê¸°ì‚¬": ["ì „ê¸°ì‚°ì—…ê¸°ì‚¬"],
            "ì†Œë°©ì„¤ë¹„ê¸°ì‚¬": ["ì†Œë°©ì„¤ë¹„ê¸°ì‚¬", "ì†Œë°©"],
            "ê±´ì¶•ê¸°ì‚¬": ["ê±´ì¶•ê¸°ì‚¬"],
            "í† ëª©ê¸°ì‚¬": ["í† ëª©ê¸°ì‚¬"],
            "ì‚°ì—…ì•ˆì „ê¸°ì‚¬": ["ì‚°ì—…ì•ˆì „ê¸°ì‚¬", "ì•ˆì „ê´€ë¦¬"],
            "í’ˆì§ˆê´€ë¦¬ê¸°ì‚¬": ["í’ˆì§ˆê´€ë¦¬ê¸°ì‚¬", "QC"],
            "í•œêµ­ì‚¬ëŠ¥ë ¥ê²€ì •ì‹œí—˜": ["í•œêµ­ì‚¬", "í•œëŠ¥ê²€"],
            "TOEIC": ["í† ìµ", "TOEIC"],
            "OPIC": ["ì˜¤í”½", "OPIC"],
            "ë³€í˜¸ì‚¬": ["ë³€í˜¸ì‚¬", "ì‚¬ë²•ì‹œí—˜"],
            "ì„¸ë¬´ì‚¬": ["ì„¸ë¬´ì‚¬"],
            "ë…¸ë¬´ì‚¬": ["ë…¸ë¬´ì‚¬", "ê³µì¸ë…¸ë¬´ì‚¬"],
        }

        for cert in certifications:
            keywords = cert_keywords.get(cert, [cert])
            found = False
            for kw in keywords:
                if kw.lower() in answer_lower or kw in answer:
                    matched_certs.append(cert)
                    found = True
                    break
            if not found:
                missing_certs.append(cert)

        # ìŠ¤í‚¬ ë§¤ì¹­
        skills = requirements.get("skills", [])
        matched_skills: list[str] = []
        missing_skills: list[str] = []

        # ìŠ¤í‚¬ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤í•‘
        skill_keywords: dict[str, list[str]] = {
            "ì „ëµì  ì‚¬ê³ ": ["ì „ëµ", "ì „ëµì ", "ê¸°íš", "ë¶„ì„"],
            "ë°ì´í„° ë¶„ì„": ["ë°ì´í„°", "ë¶„ì„", "í†µê³„", "ë¹…ë°ì´í„°", "AI", "ë¨¸ì‹ ëŸ¬ë‹"],
            "ë¬¸ì„œ ì‘ì„±": ["ë¬¸ì„œ", "ë³´ê³ ì„œ", "ì‘ì„±", "ê¸°íšì„œ"],
            "í”„ë ˆì  í…Œì´ì…˜": ["PT", "ë°œí‘œ", "í”„ë ˆì  í…Œì´ì…˜"],
            "MS Office í™œìš©": ["ì—‘ì…€", "Excel", "íŒŒì›Œí¬ì¸íŠ¸", "PPT", "ì›Œë“œ", "Office"],
            "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜": ["ì†Œí†µ", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ì˜ì‚¬ì†Œí†µ", "í˜‘ì—…"],
            "ë¬¸ì œí•´ê²°": ["ë¬¸ì œí•´ê²°", "í•´ê²°", "ê°œì„ "],
            "í”„ë¡œì íŠ¸ ê´€ë¦¬": ["í”„ë¡œì íŠ¸", "PM", "ì¼ì •ê´€ë¦¬"],
            "ì˜ì–´": ["ì˜ì–´", "English", "TOEIC", "OPIC", "íšŒí™”"],
            "ë¦¬ë”ì‹­": ["ë¦¬ë”ì‹­", "ë¦¬ë”", "íŒ€ì¥"],
            "íŒ€ì›Œí¬": ["íŒ€ì›Œí¬", "í˜‘ì—…", "í˜‘ë ¥", "íŒ€"],
            "Python": ["Python", "íŒŒì´ì¬"],
            "Java": ["Java", "ìë°”"],
            "SQL": ["SQL", "ë°ì´í„°ë² ì´ìŠ¤", "DB"],
            "CAD": ["CAD", "ì„¤ê³„", "AutoCAD"],
            "ERP": ["ERP", "SAP"],
            "íšŒê³„": ["íšŒê³„", "ì¬ë¬´", "ê²°ì‚°", "ì›ê°€"],
            "ì„¸ë¬´": ["ì„¸ë¬´", "ì„¸ê¸ˆ", "ë¶€ê°€ì„¸"],
            "ì¸ì‚¬ê´€ë¦¬": ["ì¸ì‚¬", "HR", "ì±„ìš©", "êµìœ¡"],
            "ë§ˆì¼€íŒ…": ["ë§ˆì¼€íŒ…", "í™ë³´", "ê´‘ê³ ", "ë¸Œëœë“œ"],
        }

        for skill in skills:
            keywords = skill_keywords.get(skill, [skill])
            found = False
            for kw in keywords:
                if kw.lower() in answer_lower or kw in answer:
                    matched_skills.append(skill)
                    found = True
                    break
            if not found:
                missing_skills.append(skill)

        # ì „ì²´ ë§¤ì¹­ë¥  ê³„ì‚°
        total_items = len(majors) + len(certifications) + len(skills)
        matched_items = len(matched_majors) + len(matched_certs) + len(matched_skills)

        match_rate = (matched_items / total_items * 100) if total_items > 0 else 0

        # ì¶”ì²œ ë©”ì‹œì§€ ìƒì„±
        recommendation = ""
        if match_rate >= 70:
            recommendation = "ì§ë¬´ ìš”êµ¬ì‚¬í•­ì„ ì˜ ë°˜ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤."
        elif match_rate >= 40:
            missing_items = []
            if missing_certs:
                missing_items.append(f"ìê²©ì¦({', '.join(missing_certs[:2])})")
            if missing_skills:
                missing_items.append(f"ìŠ¤í‚¬({', '.join(missing_skills[:2])})")
            if missing_items:
                recommendation = f"ë‹¤ìŒ í•­ëª©ì„ ì¶”ê°€ë¡œ ì–¸ê¸‰í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤: {', '.join(missing_items)}"
        else:
            recommendation = "ì§ë¬´ ê´€ë ¨ ê²½í—˜, ìê²©ì¦, ìŠ¤í‚¬ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•´ ë³´ì„¸ìš”."

        return PositionSkillMatch(
            matched_majors=matched_majors,
            missing_majors=missing_majors,
            matched_certifications=matched_certs,
            missing_certifications=missing_certs,
            matched_skills=matched_skills,
            missing_skills=missing_skills,
            overall_match_rate=round(match_rate, 1),
            recommendation=recommendation,
        )

    def generate_sample_answers(
        self,
        interview_questions: list[InterviewQuestion],
        answer: str,
        org_data: dict[str, Any] | None,
    ) -> list[InterviewQuestion]:
        """ë©´ì ‘ ì§ˆë¬¸ì— ëŒ€í•œ ì˜ˆì‹œ ë‹µë³€ ìƒì„± (ìì†Œì„œ ê¸°ë°˜).

        Args:
            interview_questions: ë©´ì ‘ ì§ˆë¬¸ ëª©ë¡
            answer: ìì†Œì„œ ë‹µë³€ í…ìŠ¤íŠ¸
            org_data: ê¸°ê´€ ë°ì´í„°

        Returns:
            ì˜ˆì‹œ ë‹µë³€ì´ ì¶”ê°€ëœ ë©´ì ‘ ì§ˆë¬¸ ëª©ë¡
        """
        if not interview_questions or not answer:
            return interview_questions

        # ìì†Œì„œì—ì„œ êµ¬ì²´ì ì¸ ê²½í—˜/í‚¤ì›Œë“œ ì¶”ì¶œ - ìºì‹œëœ íŒ¨í„´ ì‚¬ìš©
        sentences = _SENTENCE_SPLIT_PATTERN.split(answer)

        # ìˆ˜ì¹˜ê°€ í¬í•¨ëœ ë¬¸ì¥ ì¶”ì¶œ - ìºì‹œëœ íŒ¨í„´ ì‚¬ìš©
        quantified_experiences: list[str] = []
        for sentence in sentences:
            if len(sentence) > 20 and _NUMBER_PATTERN.search(sentence):
                quantified_experiences.append(sentence.strip())

        # í™œë™/ê²½í—˜ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ - ìºì‹œëœ íŒ¨í„´ ì‚¬ìš©
        activity_experiences: list[str] = []
        for sentence in sentences:
            if len(sentence) > 30 and _EXPERIENCE_PATTERN.search(sentence):
                activity_experiences.append(sentence.strip())

        # ê¸°ê´€ ì •ë³´
        core_values = org_data.get("core_values", []) if org_data else []
        org_name = org_data.get("name", "") if org_data else ""
        mission = org_data.get("mission", "") if org_data else ""

        # ì§ˆë¬¸ë³„ ì˜ˆì‹œ ë‹µë³€ ìƒì„±
        updated_questions: list[InterviewQuestion] = []

        for q in interview_questions:
            sample = ""
            q_text = q.question

            # ì§€ì›ë™ê¸° ì§ˆë¬¸
            if "ì§€ì›" in q_text and ("ë™ê¸°" in q_text or "ì´ìœ " in q_text or "ì™œ" in q_text):
                sample = self._generate_motivation_sample(org_name, core_values, mission, activity_experiences)

            # ì¥ë‹¨ì  ì§ˆë¬¸
            elif "ì¥ë‹¨ì " in q_text or ("ì¥ì " in q_text and "ë‹¨ì " in q_text):
                sample = self._generate_strength_weakness_sample(quantified_experiences, activity_experiences)

            # ê²½í—˜/ì‚¬ë¡€ ì§ˆë¬¸ (STAR ê¸°ë²•)
            elif any(kw in q_text for kw in ["ê²½í—˜", "ì‚¬ë¡€", "í•´ê²°", "ê·¹ë³µ", "ì–´ë ¤ì›€", "ë¬¸ì œ"]):
                sample = self._generate_star_sample(q_text, quantified_experiences, activity_experiences)

            # ì…ì‚¬ í›„ í¬ë¶€
            elif "ì…ì‚¬" in q_text and ("í¬ë¶€" in q_text or "ê³„íš" in q_text or "ëª©í‘œ" in q_text):
                sample = self._generate_aspiration_sample(org_name, mission, activity_experiences)

            # ê°ˆë“±/í˜‘ë ¥ ì§ˆë¬¸
            elif any(kw in q_text for kw in ["ê°ˆë“±", "ì˜ê²¬", "ì¶©ëŒ", "í˜‘ë ¥", "íŒ€ì›Œí¬"]):
                sample = self._generate_conflict_sample(activity_experiences)

            # ë¦¬ë”ì‹­ ì§ˆë¬¸
            elif any(kw in q_text for kw in ["ë¦¬ë”", "ì´ëŒ", "ì£¼ë„"]):
                sample = self._generate_leadership_sample(quantified_experiences, activity_experiences)

            # ê¸°íƒ€ - êµ¬ì²´ì  ë‹µë³€ ì œì‹œ
            else:
                sample = self._generate_general_sample(q_text, quantified_experiences, activity_experiences, org_name)

            # ìƒˆ ê°ì²´ ìƒì„±
            updated_questions.append(InterviewQuestion(
                question=q.question,
                is_frequent=q.is_frequent,
                years=q.years,
                answer_tips=q.answer_tips,
                sample_answer=sample,
            ))

        return updated_questions

    def _generate_motivation_sample(
        self, org_name: str, core_values: list[str], mission: str, experiences: list[str]
    ) -> str:
        """ì§€ì›ë™ê¸° ì˜ˆì‹œ ë‹µë³€ ìƒì„±."""
        sample = ""
        if org_name:
            sample = f'"{org_name}ì´(ê°€) ì¶”êµ¬í•˜ëŠ” '
            if core_values:
                sample += f"'{core_values[0]}' ê°€ì¹˜ëŠ” "
            elif mission:
                sample += f"'{mission[:30]}...' ë¯¸ì…˜ì€ "
            else:
                sample += "ë¹„ì „ì€ "

        if experiences:
            exp = experiences[0][:80] + ("..." if len(experiences[0]) > 80 else "")
            sample += f'ì œê°€ {exp} ê³¼ì •ì—ì„œ ëŠê¼ˆë˜ ê°€ì¹˜ê´€ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤. '
        else:
            sample += "ì œê°€ ì¶”êµ¬í•˜ëŠ” ë°©í–¥ê³¼ ì¼ì¹˜í•©ë‹ˆë‹¤. "

        sample += 'ì´ëŸ¬í•œ ì´ìœ ë¡œ ì´ ì¡°ì§ì—ì„œ ì „ë¬¸ì„±ì„ ìŒ“ê³  ê¸°ì—¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤."'
        return sample

    def _generate_strength_weakness_sample(
        self, quantified: list[str], activities: list[str]
    ) -> str:
        """ì¥ë‹¨ì  ì˜ˆì‹œ ë‹µë³€ ìƒì„±."""
        sample = '"ì €ì˜ ê°•ì ì€ ëˆê¸° ìˆê²Œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. '
        if quantified:
            exp = quantified[0][:70] + ("..." if len(quantified[0]) > 70 else "")
            sample += f"ì‹¤ì œë¡œ {exp} ê²°ê³¼ë¥¼ ì´ëŒì–´ëƒˆìŠµë‹ˆë‹¤. "
        elif activities:
            exp = activities[0][:70] + ("..." if len(activities[0]) > 70 else "")
            sample += f"ì˜ˆë¥¼ ë“¤ì–´ {exp} ê³¼ì •ì—ì„œ ì´ë¥¼ ì¦ëª…í–ˆìŠµë‹ˆë‹¤. "

        sample += 'ë‹¨ì ì€ ë•Œë¡œ ì„¸ë¶€ì‚¬í•­ì— ì§‘ì°©í•˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë‚˜, ìµœê·¼ì—ëŠ” ìš°ì„ ìˆœìœ„ë¥¼ ë¨¼ì € ì •í•˜ëŠ” ìŠµê´€ìœ¼ë¡œ ê°œì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤."'
        return sample

    def _generate_star_sample(
        self, question: str, quantified: list[str], activities: list[str]
    ) -> str:
        """STAR ê¸°ë²• ê¸°ë°˜ ì˜ˆì‹œ ë‹µë³€ ìƒì„±."""
        # ì§ˆë¬¸ì— ë”°ë¼ ì ì ˆí•œ ê²½í—˜ ì„ íƒ
        relevant_exp = ""
        if quantified:
            relevant_exp = quantified[0]
        elif activities:
            relevant_exp = activities[0]

        if relevant_exp:
            exp_short = relevant_exp[:60] + ("..." if len(relevant_exp) > 60 else "")
            sample = f'"[ìƒí™©] {exp_short}ì„(ë¥¼) ìˆ˜í–‰í•  ë•Œ, '
            sample += '[ê³¼ì œ] ê¸°í•œ ë‚´ ëª©í‘œ ë‹¬ì„±ì´ í•„ìš”í–ˆìŠµë‹ˆë‹¤. '
            sample += '[í–‰ë™] ì €ëŠ” ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ê³  ë‹¨ê³„ë³„ë¡œ ì ‘ê·¼í–ˆìœ¼ë©°, '
            sample += '[ê²°ê³¼] ì„±ê³µì ìœ¼ë¡œ ì™„ìˆ˜í•˜ì—¬ íŒ€ì— ê¸°ì—¬í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤."'
        else:
            sample = '"[ìƒí™©] êµ¬ì²´ì ì¸ ë°°ê²½ ì„¤ëª… â†’ [ê³¼ì œ] í•´ê²°í•´ì•¼ í•  ë¬¸ì œ â†’ [í–‰ë™] ë³¸ì¸ì´ ì·¨í•œ êµ¬ì²´ì  í–‰ë™ â†’ [ê²°ê³¼] ì •ëŸ‰ì  ì„±ê³¼ë‚˜ ë°°ìš´ ì ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”."'

        return sample

    def _generate_aspiration_sample(
        self, org_name: str, mission: str, experiences: list[str]
    ) -> str:
        """ì…ì‚¬ í›„ í¬ë¶€ ì˜ˆì‹œ ë‹µë³€ ìƒì„±."""
        sample = '"ì…ì‚¬ í›„ 1-2ë…„ê°„ì€ '
        if org_name:
            sample += f'{org_name}ì˜ í•µì‹¬ ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ë¥¼ ìµíˆë©° ì‹¤ë¬´ ì—­ëŸ‰ì„ ìŒ“ê² ìŠµë‹ˆë‹¤. '
        else:
            sample += "ì‹¤ë¬´ ì—­ëŸ‰ì„ ìŒ“ê³  ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ë¥¼ ìµíˆê² ìŠµë‹ˆë‹¤. "

        if experiences:
            exp = experiences[0][:50] + ("..." if len(experiences[0]) > 50 else "")
            sample += f"ì´ì „ {exp} ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ "

        sample += '3-5ë…„ ì°¨ì—ëŠ” ë‹´ë‹¹ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë¡œ ì„±ì¥í•˜ì—¬ í›„ë°° ì–‘ì„±ì—ë„ ê¸°ì—¬í•˜ê³ , '
        sample += 'ì¥ê¸°ì ìœ¼ë¡œëŠ” ì¡°ì§ì˜ í•µì‹¬ ì¸ì¬ë¡œì„œ ë°œì „ì— ì´ë°”ì§€í•˜ê² ìŠµë‹ˆë‹¤."'
        return sample

    def _generate_conflict_sample(self, activities: list[str]) -> str:
        """ê°ˆë“± í•´ê²° ì˜ˆì‹œ ë‹µë³€ ìƒì„±."""
        sample = '"íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ë°©í–¥ì„±ì— ëŒ€í•œ ì˜ê²¬ ì°¨ì´ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. '
        if activities:
            exp = activities[0][:50] + ("..." if len(activities[0]) > 50 else "")
            sample += f'{exp} ì§„í–‰ ì¤‘, '

        sample += 'ì €ëŠ” ë¨¼ì € ê°ìì˜ ì˜ê²¬ì„ ì¶©ë¶„íˆ ê²½ì²­í•œ í›„, '
        sample += 'ê³µí†µ ëª©í‘œë¥¼ ì¬í™•ì¸í•˜ëŠ” ì‹œê°„ì„ ê°€ì¡ŒìŠµë‹ˆë‹¤. '
        sample += 'ì´í›„ ì¥ë‹¨ì ì„ ê°ê´€ì ìœ¼ë¡œ ë¹„êµí•˜ì—¬ í•©ì˜ì ì„ ë„ì¶œí–ˆê³ , '
        sample += 'ê²°ê³¼ì ìœ¼ë¡œ ëª¨ë‘ê°€ ë§Œì¡±í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ ì™„ìˆ˜í–ˆìŠµë‹ˆë‹¤."'
        return sample

    def _generate_leadership_sample(
        self, quantified: list[str], activities: list[str]
    ) -> str:
        """ë¦¬ë”ì‹­ ì˜ˆì‹œ ë‹µë³€ ìƒì„±."""
        sample = '"'
        if activities:
            exp = activities[0][:60] + ("..." if len(activities[0]) > 60 else "")
            sample += f'{exp}ì—ì„œ íŒ€ì„ ì´ëˆ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤. '
        else:
            sample += 'íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ë¦¬ë”ë¥¼ ë§¡ì€ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤. '

        sample += 'ì €ëŠ” ë¨¼ì € íŒ€ì›ë“¤ì˜ ê°•ì ì„ íŒŒì•…í•˜ì—¬ ì ì¬ì ì†Œì— ì—­í• ì„ ë°°ë¶„í–ˆê³ , '
        sample += 'ì •ê¸°ì ì¸ ì§„í–‰ ìƒí™© ê³µìœ ë¥¼ í†µí•´ ì¼ì •ì„ ê´€ë¦¬í–ˆìŠµë‹ˆë‹¤. '

        if quantified:
            exp = quantified[0][:40] + ("..." if len(quantified[0]) > 40 else "")
            sample += f'ê·¸ ê²°ê³¼ {exp} ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."'
        else:
            sample += 'ê·¸ ê²°ê³¼ ëª©í‘œë¥¼ ì„±ê³µì ìœ¼ë¡œ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."'

        return sample

    def _generate_general_sample(
        self, question: str, quantified: list[str], activities: list[str], org_name: str
    ) -> str:
        """ì¼ë°˜ ì§ˆë¬¸ ì˜ˆì‹œ ë‹µë³€ ìƒì„±."""
        sample = '"'

        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œí•˜ì—¬ ê´€ë ¨ ê²½í—˜ ì—°ê²°
        if quantified:
            exp = quantified[0][:70] + ("..." if len(quantified[0]) > 70 else "")
            sample += f'{exp}ì˜ ê²½í—˜ì„ í†µí•´ '
        elif activities:
            exp = activities[0][:70] + ("..." if len(activities[0]) > 70 else "")
            sample += f'{exp} ê³¼ì •ì—ì„œ '
        else:
            sample += 'ê´€ë ¨ ê²½í—˜ì„ í†µí•´ '

        sample += 'ì´ ì—­ëŸ‰ì„ í‚¤ì›Œì™”ìŠµë‹ˆë‹¤. '
        if org_name:
            sample += f'{org_name}ì—ì„œ ì´ëŸ¬í•œ ì—­ëŸ‰ì„ ë°œíœ˜í•˜ì—¬ '
        sample += 'ì¡°ì§ì— ê¸°ì—¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤."'

        return sample

    def _format_model_answer_paragraphs(self, model_answer: str) -> str:
        """ëª¨ë²” ë‹µì•ˆì— ë¬¸ë‹¨ êµ¬ë¶„(ì¤„ë°”ê¿ˆ)ì„ ì¶”ê°€.

        LLMì´ ì¤„ë°”ê¿ˆ ì—†ì´ ì—°ì†ëœ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ ê²½ìš°,
        ì ì ˆí•œ ìœ„ì¹˜ì— ë¬¸ë‹¨ êµ¬ë¶„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

        Args:
            model_answer: ì›ë³¸ ëª¨ë²” ë‹µì•ˆ

        Returns:
            ë¬¸ë‹¨ì´ êµ¬ë¶„ëœ ëª¨ë²” ë‹µì•ˆ
        """
        if not model_answer:
            return model_answer

        # ì´ë¯¸ ì¤„ë°”ê¿ˆì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if "\n" in model_answer:
            return model_answer

        # ë¬¸ë‹¨ êµ¬ë¶„ ê¸°ì¤€ íŒ¨í„´ (ìƒˆë¡œìš´ ë¬¸ë‹¨ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ë“¤)
        paragraph_starters = [
            "ì´ëŸ¬í•œ ê²½í—˜",
            "ì´ë¥¼ ê³„ê¸°ë¡œ",
            "ì´í›„",
            "ë˜í•œ",
            "í•œí¸",
            "ë‚˜ì•„ê°€",
            "ì•ìœ¼ë¡œ",
            "ì…ì‚¬ í›„",
            "íŠ¹íˆ",
            "ê²°ê³¼ì ìœ¼ë¡œ",
            "ì´ë¥¼ í†µí•´",
            "ì´ì²˜ëŸ¼",
        ]

        result = model_answer
        for starter in paragraph_starters:
            # ë¬¸ì¥ ì¤‘ê°„ì´ ì•„ë‹Œ ìƒˆë¡œìš´ ë¬¸ì¥ ì‹œì‘ì—ì„œë§Œ êµ¬ë¶„
            # ". ì´ëŸ¬í•œ" ë˜ëŠ” "ë‹¤. ì´ëŸ¬í•œ" íŒ¨í„´ì„ ì°¾ì•„ì„œ ì¤„ë°”ê¿ˆ ì¶”ê°€
            import re
            pattern = rf'([.!?])\s+({starter})'
            result = re.sub(pattern, r'\1\n\n\2', result)

        return result

    def _get_default_interview_tips(self, category: str, question: str = "") -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ë©´ì ‘ ì¡°ì–¸ ìƒì„±.

        Args:
            category: ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ (ìê¸°ì†Œê°œ, ì§€ì›ë™ê¸° ë“±)
            question: ì›ë³¸ ì§ˆë¬¸ (ìƒì„¸ íŒ ìƒì„±ìš©)

        Returns:
            í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ë©´ì ‘ ì¡°ì–¸
        """
        # ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ì¡°ì–¸ ë§¤í•‘
        tips_map = {
            "ìê¸°ì†Œê°œ": (
                "1ë¶„ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì¤€ë¹„í•˜ì„¸ìš”. "
                "í•µì‹¬ ê²½í—˜ 1-2ê°œì™€ ì§€ì› ì§ë¬´ì™€ì˜ ì—°ê²°ì ì„ ê°•ì¡°í•˜ê³ , "
                "ë§ˆì§€ë§‰ì— ì…ì‚¬ í›„ í¬ë¶€ë¥¼ ì§§ê²Œ ë§ë¶™ì´ì„¸ìš”."
            ),
            "ì§€ì›ë™ê¸°": (
                "ê¸°ê´€ì˜ ë¯¸ì…˜/ë¹„ì „ê³¼ ë³¸ì¸ì˜ ê°€ì¹˜ê´€ ì—°ê²°ì´ í•µì‹¬ì…ë‹ˆë‹¤. "
                "í•´ë‹¹ ê¸°ê´€ë§Œì˜ ì°¨ë³„ì (ì‚¬ì—…, ì •ì±…)ì„ ì–¸ê¸‰í•˜ê³ , "
                "êµ¬ì²´ì  ê¸°ì—¬ ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”."
            ),
            "ê¸°ê´€ì´í•´": (
                "ì£¼ìš” ì‚¬ì—… 3ê°€ì§€ ì´ìƒ ìˆ™ì§€í•˜ì„¸ìš”. "
                "ìµœê·¼ ë‰´ìŠ¤, ì •ì±… ë°©í–¥, ì‹ ì‚¬ì—…ì„ íŒŒì•…í•˜ê³ , "
                "ì§€ì› ì§ë¬´ì™€ ì—°ê³„í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”."
            ),
            "ì§ë¬´ì—­ëŸ‰": (
                "STAR ê¸°ë²•(ìƒí™©-ê³¼ì œ-í–‰ë™-ê²°ê³¼)ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
                "ì •ëŸ‰ì  ì„±ê³¼(ìˆ«ì, ë¹„ìœ¨)ë¥¼ í¬í•¨í•˜ê³ , "
                "í•´ë‹¹ ê²½í—˜ì´ ì§ë¬´ì— ì–´ë–»ê²Œ í™œìš©ë ì§€ ì—°ê²°í•˜ì„¸ìš”."
            ),
            "ê²½í—˜/ì—­ëŸ‰": (
                "STAR ê¸°ë²•ìœ¼ë¡œ êµ¬ì²´ì  ì‚¬ë¡€ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”. "
                "íŒ€ í”„ë¡œì íŠ¸ì—ì„œì˜ ì—­í• ê³¼ ê¸°ì—¬ë¥¼ ëª…í™•íˆ í•˜ê³ , "
                "ë°°ìš´ ì ê³¼ ì„±ì¥ í¬ì¸íŠ¸ë¥¼ ê°•ì¡°í•˜ì„¸ìš”."
            ),
            "ì¸ì„±/ê°€ì¹˜ê´€": (
                "ì†”ì§í•˜ë˜, ì§ë¬´ ì—°ê´€ì„±ì„ ê³ ë ¤í•˜ì„¸ìš”. "
                "ë‹¨ì  ì§ˆë¬¸ì‹œ ê·¹ë³µ ë…¸ë ¥ê³¼ ê°œì„  ê²°ê³¼ë¥¼ í•¨ê»˜ ì–¸ê¸‰í•˜ê³ , "
                "êµ¬ì²´ì  ì—í”¼ì†Œë“œë¡œ ì§„ì •ì„±ì„ ë³´ì—¬ì£¼ì„¸ìš”."
            ),
            "ë¬¸ì œí•´ê²°": (
                "ë¬¸ì œ ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , "
                "ë³¸ì¸ì˜ ë¶„ì„ ê³¼ì •ê³¼ í•´ê²° ë°©ì•ˆì„ ë‹¨ê³„ë³„ë¡œ ì œì‹œí•˜ì„¸ìš”. "
                "ê²°ê³¼ì™€ í•¨ê»˜ ë°°ìš´ êµí›ˆì„ ë§ë¶™ì´ì„¸ìš”."
            ),
            "ì¡°ì§ì í•©ì„±": (
                "ê¸°ê´€ì˜ í•µì‹¬ê°€ì¹˜ì™€ ì¸ì¬ìƒì„ ìˆ™ì§€í•˜ì„¸ìš”. "
                "ë³¸ì¸ì˜ ê²½í—˜ ì¤‘ í•´ë‹¹ ê°€ì¹˜ë¥¼ ì‹¤ì²œí•œ ì‚¬ë¡€ë¥¼ ì¤€ë¹„í•˜ê³ , "
                "ì¡°ì§ ë¬¸í™”ì— ì ì‘í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì„¸ìš”."
            ),
            "ë°œí‘œ": (
                "ì„œë¡ -ë³¸ë¡ -ê²°ë¡  êµ¬ì¡°ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”. "
                "í•µì‹¬ ë©”ì‹œì§€ 1-2ê°œì— ì§‘ì¤‘í•˜ê³ , "
                "ì‹œê°„ ë°°ë¶„(ì¤€ë¹„ 15ë¶„, ë°œí‘œ 5ë¶„ ë“±)ì„ ì² ì €íˆ ì§€í‚¤ì„¸ìš”."
            ),
            "í† ë¡ ": (
                "ì°¬ë°˜ ì–‘ìª½ ë…¼ê±°ë¥¼ ë¯¸ë¦¬ ì¤€ë¹„í•˜ì„¸ìš”. "
                "ìƒëŒ€ ì˜ê²¬ì„ ê²½ì²­í•˜ê³  ì¡´ì¤‘í•˜ëŠ” íƒœë„ë¥¼ ë³´ì´ë©°, "
                "ë…¼ë¦¬ì  ê·¼ê±°ì™€ ë°ì´í„°ë¡œ ì£¼ì¥ì„ ë’·ë°›ì¹¨í•˜ì„¸ìš”."
            ),
            "ìƒí™©ëŒ€ì²˜": (
                "ì¹¨ì°©í•˜ê²Œ ìƒí™©ì„ íŒŒì•…í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì£¼ì„¸ìš”. "
                "ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ê³  ë‹¨ê³„ì  í•´ê²° ë°©ì•ˆì„ ì œì‹œí•˜ë©°, "
                "ìœ ì‚¬ ê²½í—˜ì´ ìˆë‹¤ë©´ í•¨ê»˜ ì–¸ê¸‰í•˜ì„¸ìš”."
            ),
            "ë¹„ì „": (
                "ë‹¨ê¸°(1-2ë…„)ì™€ ì¥ê¸°(5-10ë…„) ëª©í‘œë¥¼ êµ¬ë¶„í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”. "
                "ê¸°ê´€ì˜ ì‚¬ì—… ë°©í–¥ê³¼ ì—°ê³„ëœ êµ¬ì²´ì  ì„±ì¥ ê³„íšì„ ë§í•˜ê³ , "
                "í•´ë‹¹ ê¸°ê´€ì—ì„œ ì´ë£¨ê³  ì‹¶ì€ ì „ë¬¸ì„±ì„ ëª…í™•íˆ í•˜ì„¸ìš”."
            ),
            "í¬ë¶€": (
                "ì…ì‚¬ í›„ 1-2ë…„ ë‚´ ë‹¬ì„±í•  êµ¬ì²´ì  ëª©í‘œë¥¼ ì œì‹œí•˜ì„¸ìš”. "
                "ê¸°ê´€ì˜ ë¯¸ì…˜ê³¼ ì—°ê³„ëœ ê¸°ì—¬ ë°©ì•ˆì„ ì„¤ëª…í•˜ê³ , "
                "í•´ë‹¹ ì§ë¬´ì—ì„œ ë°œíœ˜í•  ì—­ëŸ‰ì„ ê°•ì¡°í•˜ì„¸ìš”."
            ),
            "ì¸ì„±": (
                "ì†”ì§í•˜ë˜, ì§ë¬´ ì—°ê´€ì„±ì„ ê³ ë ¤í•˜ì„¸ìš”. "
                "ë‹¨ì  ì§ˆë¬¸ì‹œ ê·¹ë³µ ë…¸ë ¥ê³¼ ê°œì„  ê²°ê³¼ë¥¼ í•¨ê»˜ ì–¸ê¸‰í•˜ê³ , "
                "êµ¬ì²´ì  ì—í”¼ì†Œë“œë¡œ ì§„ì •ì„±ì„ ë³´ì—¬ì£¼ì„¸ìš”."
            ),
            "ìê¸°ë¶„ì„": (
                "ê°ê´€ì ì¸ ìê¸° ë¶„ì„ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì„¸ìš”. "
                "ê°•ì ì€ êµ¬ì²´ì  ì‚¬ë¡€ë¡œ, ì•½ì ì€ ê°œì„  ë…¸ë ¥ê³¼ í•¨ê»˜ ì–¸ê¸‰í•˜ê³ , "
                "ì§€ì› ì§ë¬´ì™€ì˜ ì—°ê´€ì„±ì„ í•­ìƒ ê³ ë ¤í•˜ì„¸ìš”."
            ),
            "ê²½í—˜": (
                "STAR ê¸°ë²•ìœ¼ë¡œ êµ¬ì²´ì  ì‚¬ë¡€ë¥¼ ì¤€ë¹„í•˜ì„¸ìš”. "
                "íŒ€ í”„ë¡œì íŠ¸ì—ì„œì˜ ì—­í• ê³¼ ê¸°ì—¬ë¥¼ ëª…í™•íˆ í•˜ê³ , "
                "ë°°ìš´ ì ê³¼ ì„±ì¥ í¬ì¸íŠ¸ë¥¼ ê°•ì¡°í•˜ì„¸ìš”."
            ),
        }

        # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ (ë¶€ë¶„ ì¼ì¹˜ í—ˆìš©)
        for key, tip in tips_map.items():
            if key in category or category in key:
                return tip

        # ì§ˆë¬¸ ë‚´ìš© ê¸°ë°˜ ì¶”ê°€ ë§¤ì¹­
        if "ì¥ë‹¨ì " in question or "ë‹¨ì " in question or "ì¥ì " in question:
            return tips_map.get("ì¸ì„±/ê°€ì¹˜ê´€", "")
        if "ê°ˆë“±" in question or "í˜‘ì—…" in question or "íŒ€" in question:
            return (
                "ê°ˆë“± ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , "
                "ë³¸ì¸ì˜ ì¤‘ì¬/í•´ê²° ë…¸ë ¥ì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”. "
                "ê²°ê³¼ì ìœ¼ë¡œ íŒ€ì— ë¯¸ì¹œ ê¸ì •ì  ì˜í–¥ì„ ê°•ì¡°í•˜ì„¸ìš”."
            )
        if "í¬ë¶€" in question or "ì…ì‚¬ í›„" in question or "ëª©í‘œ" in question:
            return (
                "ë‹¨ê¸°(1-2ë…„)ì™€ ì¥ê¸°(5ë…„) ëª©í‘œë¥¼ êµ¬ë¶„í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”. "
                "ê¸°ê´€ì˜ ì‚¬ì—… ë°©í–¥ê³¼ ì—°ê³„ëœ êµ¬ì²´ì  ê³„íšì„ ë§í•˜ê³ , "
                "ì‹¤í˜„ ê°€ëŠ¥í•œ í˜„ì‹¤ì ì¸ ëª©í‘œë¥¼ ì„¤ì •í•˜ì„¸ìš”."
            )

        # ê¸°ë³¸ ì¡°ì–¸
        return (
            "ì§ˆë¬¸ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. "
            "êµ¬ì²´ì  ê²½í—˜ê³¼ ì‚¬ë¡€ë¡œ ë‹µë³€ì— ì‹ ë¢°ì„±ì„ ë”í•˜ê³ , "
            "ì§€ì› ê¸°ê´€/ì§ë¬´ì™€ì˜ ì—°ê´€ì„±ì„ í•­ìƒ ê³ ë ¤í•˜ì„¸ìš”."
        )

    def _get_interview_detail(self, interview_data: dict[str, Any] | None) -> InterviewDetailInfo:
        """ë©´ì ‘ ìƒì„¸ ì •ë³´ ì¶”ì¶œ.

        Args:
            interview_data: ë©´ì ‘ ë°ì´í„°

        Returns:
            ë©´ì ‘ ìƒì„¸ ì •ë³´
        """
        if not interview_data:
            return InterviewDetailInfo()

        interview_format = interview_data.get("interview_format", {})

        # ê³ ë¹ˆë„ ì§ˆë¬¸ ì¶”ì¶œ (ìƒìœ„ 5ê°œ)
        questions = interview_data.get("questions", [])
        frequent_questions: list[FrequentInterviewQuestion] = []

        # frequencyê°€ highì¸ ê²ƒ ìš°ì„ , ê·¸ ë‹¤ìŒ medium
        high_freq = [q for q in questions if q.get("frequency") == "high"]
        medium_freq = [q for q in questions if q.get("frequency") == "medium"]

        for q in (high_freq + medium_freq)[:5]:
            question_text = q.get("question", "")
            category = q.get("category", "")
            existing_tips = q.get("tips", "")

            # ê¸°ì¡´ íŒì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ (30ì ë¯¸ë§Œ) ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê¸°ë³¸ íŒ ìƒì„±
            if not existing_tips or len(existing_tips) < 30:
                existing_tips = self._get_default_interview_tips(category, question_text)

            frequent_questions.append(FrequentInterviewQuestion(
                question=question_text,
                category=category,
                frequency=q.get("frequency", "medium"),
                tips=existing_tips,
            ))

        return InterviewDetailInfo(
            format_type=interview_format.get("type", ""),
            stages=interview_format.get("stages", []),
            duration=interview_format.get("duration", ""),
            difficulty=interview_format.get("difficulty", ""),
            pass_rate=interview_format.get("positive_rate", ""),
            frequent_questions=frequent_questions,
        )

    def _parse_response_v2(
        self,
        llm_response: str,
        request: ResumeAnalysisRequest,
        org_data: dict[str, Any] | None = None,
        interview_data: dict[str, Any] | None = None,
        past_questions_data: list[dict[str, Any]] | None = None,
        core_value_scores: list[CoreValueScore] | None = None,
        ncs_competency_scores: list[NCSCompetencyScore] | None = None,
        position_skill_match: PositionSkillMatch | None = None,
    ) -> NewResumeAnalysisResponse:
        """Parse LLM response into new structured response format.

        Args:
            llm_response: Raw LLM JSON response
            request: Original request for length calculation
            org_data: Organization data from knowledge DB
            interview_data: Interview data from knowledge DB
            past_questions_data: Past essay questions from knowledge DB

        Returns:
            Validated new response object
        """
        try:
            data = json.loads(llm_response)
        except json.JSONDecodeError as e:
            logger.error(f"LLM returned invalid JSON (v2): {e}")
            logger.debug(f"Raw response: {llm_response[:500]}...")
            # Return default response structure on parse error
            data = {
                "overall_score": 50,
                "overall_grade": "ë³´í†µ",
                "overall_summary": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "strengths": [],
                "improvements": [],
                "keyword_analysis": {"found_keywords": [], "missing_keywords": [], "match_rate": 0},
                "interview_questions": [],
                "model_answer_checklist": {},
                "model_answer": "",
                "model_answer_length": 0,
            }

        # Calculate actual length check
        length_check = self.calculate_length_check(request.answer, request.maxLength)

        # Build organization info directly from knowledge DB (NOT from LLM)
        recent_news_raw: list[dict[str, Any]] = []
        if org_data:
            # ì±„ìš© ë‰´ìŠ¤
            for news in org_data.get("recruitment_news", []):
                recent_news_raw.append({
                    "title": news.get("title", ""),
                    "date": news.get("date", ""),
                    "category": "ì±„ìš©",
                    "url": news.get("url", ""),
                })
            # ì‚¬ì—… ë‰´ìŠ¤
            for news in org_data.get("business_news", []):
                recent_news_raw.append({
                    "title": news.get("title", ""),
                    "date": news.get("date", ""),
                    "category": "ì‚¬ì—…",
                    "url": news.get("url", ""),
                })

        # Sort by date descending (most recent first)
        def parse_date_key(item: dict[str, Any]) -> str:
            date_str = item.get("date", "")
            # Handle various date formats: "2025-09", "2025", "2025-04"
            if not date_str:
                return "0000-00"
            # Normalize to "YYYY-MM" format for sorting
            if len(date_str) == 4:  # "2025" -> "2025-12" (assume end of year)
                return f"{date_str}-12"
            elif len(date_str) == 7:  # "2025-09" already correct
                return date_str
            return date_str[:7] if len(date_str) >= 7 else f"{date_str}-00"

        recent_news_raw.sort(key=parse_date_key, reverse=True)

        # Convert to RecentNewsItem (max 5)
        recent_news = [
            RecentNewsItem(
                title=item["title"],
                date=item["date"],
                category=item["category"],
                url=item.get("url", ""),
            )
            for item in recent_news_raw[:5]
        ]

        # Get info from org_data (knowledge DB)
        website = ""
        recruitment_process = []
        data_updated_at = ""
        core_values = []
        talent_image = ""
        org_name = ""
        if org_data:
            org_name = org_data.get("name", "")
            website = org_data.get("website", "")
            core_values = org_data.get("core_values", [])
            talent_image = org_data.get("talent_image", "")
            recruitment_process = org_data.get("recruitment", {}).get("process", [])
            metadata = org_data.get("metadata", {})
            data_updated_at = metadata.get("last_updated", "")

        # Get interview keywords and statistics from interview_data
        interview_keywords = []
        interview_difficulty = ""
        interview_pass_rate = ""
        if interview_data:
            interview_format = interview_data.get("interview_format", {})
            interview_difficulty = interview_format.get("difficulty", "")
            interview_pass_rate = interview_format.get("positive_rate", "")

            # Extract keywords from high-frequency questions
            questions = interview_data.get("questions", [])
            keyword_set: set[str] = set()
            for q in questions:
                if q.get("frequency") in ("high", "medium"):
                    cat = q.get("category", "")
                    if cat:
                        keyword_set.add(cat)
                for ncs in q.get("ncs_competencies", [])[:2]:
                    keyword_set.add(ncs)
            interview_keywords = list(keyword_set)[:8]

        organization_info = OrganizationInfo(
            name=org_name,
            website=website,
            core_values=core_values,
            talent_image=talent_image,
            recent_news=recent_news,
            interview_keywords=interview_keywords,
            recruitment_process=recruitment_process,
            interview_difficulty=interview_difficulty,
            interview_pass_rate=interview_pass_rate,
            data_updated_at=data_updated_at,
        )

        # Parse strengths
        strengths = [
            StrengthItem(
                title=s.get("title", ""),
                score=s.get("score", 5),
                quote=s.get("quote", ""),
                evaluation=s.get("evaluation", ""),
            )
            for s in data.get("strengths", [])
        ]

        # Parse improvements
        improvements = [
            ImprovementItem(
                title=i.get("title", ""),
                score=i.get("score", 5),
                problem=i.get("problem", ""),
                current_text=i.get("current_text", ""),
                improved_text=i.get("improved_text", ""),
            )
            for i in data.get("improvements", [])
        ]

        # Parse keyword analysis
        keyword_data = data.get("keyword_analysis", {})
        keyword_analysis = KeywordAnalysis(
            found_keywords=keyword_data.get("found_keywords", []),
            missing_keywords=keyword_data.get("missing_keywords", []),
            match_rate=keyword_data.get("match_rate", 0),
        )

        # Parse interview questions with multiple years support
        interview_questions = [
            InterviewQuestion(
                question=q.get("question", ""),
                is_frequent=q.get("is_frequent", False),
                years=q.get("years", []) if isinstance(q.get("years"), list) else ([q.get("year")] if q.get("year") else []),
                answer_tips=q.get("answer_tips", ""),
            )
            for q in data.get("interview_questions", [])
        ]

        # Parse past questions from knowledge DB
        past_questions = []
        if past_questions_data:
            past_questions = [
                PastQuestion(
                    year=pq.get("year", 0),
                    half=pq.get("half", ""),
                    question=pq.get("question", ""),
                    char_limit=pq.get("char_limit", 0),
                    is_prediction=pq.get("is_prediction", False),
                )
                for pq in past_questions_data
            ]

        # Get model answer checklist (internal use for quality assurance)
        model_answer_checklist = data.get("model_answer_checklist", {})

        # Get model answer - always calculate actual length (don't trust LLM's count)
        model_answer = data.get("model_answer", "")
        # ë¬¸ë‹¨ êµ¬ë¶„ í›„ì²˜ë¦¬: LLMì´ ì¤„ë°”ê¿ˆì„ ë„£ì§€ ì•Šì€ ê²½ìš° ìë™ìœ¼ë¡œ ë¬¸ë‹¨ êµ¬ë¶„
        model_answer = self._format_model_answer_paragraphs(model_answer)
        # Calculate actual length (excluding whitespace at start/end but keeping internal)
        model_answer_length = len(model_answer.strip())

        # Check for common mistakes/warnings
        warnings = self.check_warnings(request.answer, request.organization)

        # Get interview detail info
        interview_detail = self._get_interview_detail(interview_data)

        # Use pre-computed core value scores or compute if not provided
        if core_value_scores is None:
            core_value_scores = self.analyze_core_values(request.answer, core_values)

        # Use pre-computed NCS competency scores or compute if not provided
        if ncs_competency_scores is None:
            ncs_competency_scores = self.analyze_ncs_competencies(request.answer, request.position)

        # Find similar past questions
        similar_questions = self.find_similar_questions(request.question, request.organization)

        # Use pre-computed position skill match or compute if not provided
        if position_skill_match is None:
            position_skill_match = self.analyze_position_skills(request.answer, request.position)

        # Generate sample answers for interview questions
        interview_questions = self.generate_sample_answers(
            interview_questions, request.answer, org_data
        )

        return NewResumeAnalysisResponse(
            overall_score=data.get("overall_score", 50),
            overall_grade=data.get("overall_grade", "ë³´í†µ"),
            overall_summary=data.get("overall_summary", ""),
            length_check=length_check,
            warnings=warnings,
            organization_info=organization_info,
            interview_detail=interview_detail,
            strengths=strengths,
            improvements=improvements,
            keyword_analysis=keyword_analysis,
            core_value_scores=core_value_scores,
            ncs_competency_scores=ncs_competency_scores,
            similar_questions=similar_questions,
            position_skill_match=position_skill_match,
            interview_questions=interview_questions,
            past_questions=past_questions,
            model_answer_checklist=model_answer_checklist,
            model_answer=model_answer,
            model_answer_length=model_answer_length,
        )
